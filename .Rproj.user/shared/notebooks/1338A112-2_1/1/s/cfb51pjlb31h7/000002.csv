"0","```r
#Tokenisation der Tweets (1-gram) und Pre-processing mit tidytext

tweet_words <- tweets %>%
  select(username, 
         text, 
         retweetcount, 
         favorite_count,
         tweetcreatedts, 
         tweetid,
         hour) %>%
  unnest_tokens(word, text, token = \"tweets\") %>%
  anti_join(stop_words, by = \"word\") %>%
  filter(!word %in% c(\"amp\", \"de\", \"|\", \"la\", \"en\", \"die\", \"#ukraine\", \"und\", \"der\", \"le\", \"di\"),
         stringr::str_detect(word, \"[a-z]\"))
```"
"1","Using `to_lower = TRUE` with `token = 'tweets'` may not preserve URLs.
"
"0","```r
tweet_words %>%
  count(word, sort = TRUE) %>%
  head(16) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot2::ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(title = \"Most common words in Tweets on Ukraine\",
       y = \"Frequency of words\")

```"
