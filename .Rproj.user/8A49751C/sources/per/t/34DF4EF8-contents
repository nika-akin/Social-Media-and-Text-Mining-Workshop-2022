---
title: "Social Media und Text Mining Workshop with R"
output: 
  html_document:
    theme: united
    highlight: tango
    df_print: paged
    code_folding: show
---
```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, error = F)

options(digits =2, knitr.kable.NA = '')
```




#### 1. Basic workflow for working with text data

##### A) **Sampling Text Data**
- Existing Archives (e.g., [Reddit Pushift Data](https://files.pushshift.io/reddit/comments/), vgl. auch [Baumgartner et al., 2020](https://cs.paperswithcode.com/paper/the-pushshift-reddit-dataset))
- APIs (e.g.,. [The Guardian](https://cran.r-project.org/web/packages/guardianapi/guardianapi.pdf))
- Web Scraping (e.g., with [rvest](https://rvest.tidyverse.org/)) (or just for fun, build you own API with [plumber](https://www.rplumber.io/))

##### B) **Pre-processing/ Data Wrangling**
 - Cleaning
- Selecting and Weightning of *Features* (Reducing dimensionality)

##### C) **Analyses**
- *Dictionary*- or rulebased Approaches
- Supervised Approaches (e.g., SVM)
- Unsupervised Approaches (e.g., Topic Modeling)

##### D) **Validation**
- e.g., Reliability of Bot Classifications ([tweetbotornot2](https://github.com/mkearney/tweetbotornot2#botometer))


#### 2. Data Sampling

##### A) **Screen Scraping**: 
- *Scraping*, *Parsing* and *Formatting* (e.g., with *Rselenium*)


##### B) **API Access Points**: 
- Send **Get** Requests directly to the data base
- Gateways for specific data types, irrespective of the programming language 


##### B.1. Application Programming Interfaces (APIs)

- "communicates" directly with the data base
- determines *which* information are accessible for  *whom*, *how* and to which *degree*


B.2. **API-Applications**:

- Embedd content into other applications
- Build bots (e.g., [Telegram](https://core.telegram.org/bots/api))
- Collect data for market research

Access point exist for:

- [**Youtube**](https://developers.google.com/youtube/v3) 
-- allows via keywords to search for contents, the Video, lists and user activities such as upvoting, comments, favorites

- [**Instagram**](https://github.com/digitalmethodsinitiative/dmi-instascraper/) 
-- allows to search for comment structures relating to postings, friends information of users or geolocation

- [**Wikipedia**](https://cran.r-project.org/web/packages/WikipediR/WikipediR.pdf)
-- allows to search for MediaWiki revisions, revision summaries connected to an entry, timestamps, site information, user information

- [**Google Maps**](https://cran.rstudio.com/web/packages/mapsapi/vignettes/intro.html)
-- allows to search for coordinates of latitude and longitude, distance matrices


B.3. **Advantages of API harvesting**:

- No interaction with HTML data types necessary (Output: `JSON`-files)
- Usually legal (upon following the Terms of Service)


B.4. **Disadvantages of API harvesting**:

- Not every website has an API
- Only the data that the API makes available can be retrieved
- *Rate limitations* (e.g. number of tweets per day/ query).
- Terms of use and changes to the API restrict use (e.g. code reproducibility, sharing of data sets)
- Code varies depending on platform and level of documentation detail

#### 3. Tool overview for Twitter data access
Twitter for research on the dynamics of fast-moving socio-political events and contemporary culture.

Differentiation by requirements:

- Graphical User Interface
- Type of data
- Suitability for collection or for processing and analysis
- API version (e.g. `Twitter API v1.1` offer the packages `rtweet`)

**Sources for Tools**: 

- [Twitter Tool List: Wiki of the Social Media Observatory am Leibniz-Institut für Medienforschung | Hans-Bredow-Institut (HBI)](https://smo-wiki.leibniz-hbi.de/Twitter-Tools)

- Tutorial for Twitter-Daten: Jürgens, P., & Jungherr, A. (2016). A tutorial for using Twitter data in the social sciences: Data collection, preparation, and analysis. Preparation, and Analysis (January 5, 2016).)

- [Tools for Collecting and Analysing Social Media Data](https://wiki.digitalmethods.net/Dmi/ToolDatabase)


#### 4. Academic Twitter Access Point (v2 API endpoints)
- To run the following code examples, [Academic Research Access](https://developer.twitter.com/en/products/twitter-api/academic-research) for the Twitter API v2 is required. To sample Twitter data, one uses the `R` package [`academictwitteR`](https://cran.r-project.org/web/packages/academictwitteR/academictwitteR.pdf).

- In the framework, one needs the so-called [Bearer Token](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens). This [vignette](https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-auth.html) in the `academictwitteR` package explains the process of gaining access to the Twitter API.

**Sources for Academic Twitter API**:

- https://developer.twitter.com/en/use-cases/do-research/academic-research/resources
- https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-tidy.html
- https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list

#### 4.1. Twitter Modi with v2 API Endpoints

```{r Twitter academic code for tweets, warning=FALSE, eval=FALSE}
# Tweet-centric sampling (samples 1000 tweets)

get_all_tweets(
  query = 'xyz',
  start_tweets,
  end_tweets,
  bearer_token = get_bearer(),
  n = 1000
)
```

```{r Twitter academic code for User_Id, warning=FALSE, eval=FALSE}
# User-centric sampling: defining users

users <- c("juliasilge", "drob")

get_user_id(users, bearer_token)
```

```{r Twitter academic code for User Timeline, eval=FALSE}
# User-centric sampling (samples 100 tweets)

get_user_timeline(
userid, #single string or Vector with User_Ids
start_tweets,
end_tweets,
bearer_token = get_bearer(),
n = 100,
)
```

#### Mock-Sampling
**Authentication**
```{r, eval=FALSE}
#API Credentials
oautCred <- read.delim(file = "oautCred.txt", header = TRUE, sep = ",") %>% unlist()

# "Bearer Token" in Environment
Sys.setenv(TWITTER_BEARER = unname(oautCred["bearer"]))
```
**Twitter-Query**
```{r, eval=FALSE}


get_all_tweets(
  query = '(#depression) -is:retweet',                #Tweet Search Query
               n = Inf,                               #max tweet nr 
               start_tweets = "2022-08-19T00:00:00Z", #Start date
               end_tweets = "2022-08-19T23:59:59Z",   #End date
               file = "ukraine_tweets",               #file name
               data_path = "ukraine_data/",           #path to file
               bind_tweets = TRUE,                    #binding to data frame
               )


#Entfernen der API Credentials
rm(oautCred)

```


```{r, eval=FALSE}
tweets <- bind_tweets(data_path = "ukraine_data/", output_format = "tidy")
```


#### 5. Hands-on Twitter Demo
For demonstration purposes we retrieve an archived Twitter (sub-)sample, containing English **depression** specific tweets. (The sample contains *N*= 20,000 tweets.)
We retrieve a subsample from the plattform *Kaggle*.

**a.)** This can be directly downloaded: **[Kaggle Data Dump, Depression"](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv)**

**b.)** Or by issuing a `GET` request via its `API` (see below).

For obtaining the data via the `Kaggle API` we *install* and *load* all dependencies to use `kaggler`.

```{r, message=FALSE}
#install.packages(c("devtools"))
devtools::install_github("ldurazo/kaggler")
```

```{r, message=FALSE}
library(readr)
library(kaggler)
```
A `kaggle.json` file needs to be made for **authentication** on Kaggle.
The `kaggle.json` file should be positioned within the working directory. The json contains: *username* and the `API token`.

To create your `API token` go to your Kaggle Profil: (Your profile > Account > Settings > API > Create new token)

```{r eval=FALSE}
kgl_auth(creds_file = 'kaggle.json')
```

```{r eval=FALSE}
response <- kgl_datasets_download_all(owner_dataset = "infamouscoder/mental-health-social-media?select=Mental-Health-Twitter")
```


```{r eval=FALSE}
library(httr)
dataset <- httr::GET("https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv")

temp <- tempfile()
download.file(dataset$url,temp)
data <- read.csv(unz(temp, "Mental-Health-Twitter.csv"))
unlink(temp)
```

```{r eval=FALSE}
mental <- data
```

```{r message=FALSE, echo=FALSE}
setwd("C:/Users/batzdova/Desktop")
```

```{r}

library(here)
mental <- read_csv("Mental-Health-Twitter.csv")
```

Load further dependencies
#### `R`-packages
```{r, eval=TRUE, message=FALSE}
library(academictwitteR)    # collecting Twitter data
library(dplyr)              # Data Wrangling, pre-processing
library(quanteda)           # text-based descriptive
library(quanteda.textstats)
library(quanteda.textplots)
library(tidytext)           # Text Mining 'tidy format'
library(textclean)          # Pre-Processing + harmonizing
library(lubridate)          # Wrangling with time series data
library(ggplot2)

theme_set(theme_light())
```


#### Variable Overview

```{r}
glimpse(mental)
```
**Parse Date Variable** `post_created`

```{r}
# Remove the timezone offset from the date string
mental$date <- gsub(" \\+\\d{4}", "", mental$post_created)
```



```{r}
#install.packages("anytime")
library(anytime)
# Convert the date string to a date variable
mental$date_any <- anytime(mental$date)

# Format the date variable as "YYYY-MM-DD H:M"
mental$formatted_date <- format(mental$date_any , "%Y-%m-%d %H:%M")
```

`Formatted_date` is a `character` class.
When we would use `as.Date()` we would lose the `timestamp` information.
When we have a character with both date and time we have to use `POSIXct` class (as seen with the `date_any` variable).
```{r}
mental$formatted_date_time <- as.POSIXlt(mental$formatted_date)
```


**Distribution Friends**

```{r }
mental$logfriends <-log(mental$friends+1)

hist(mental$logfriends, breaks = 50)
```
**Distribution Followers**

```{r Log transformed followers}
mental$logfollow=log(mental$followers+1)
```

```{r}
hist(mental$logfollow, breaks = 50)

ggplot(mental, aes(x = logfriends, y = logfollow)) + 
  geom_point(alpha=.5)
```


**Tweets und Retweets per day**
```{r}
twt_retwt_summary <- mental %>%
  group_by(formatted_date) %>%
  summarize(tweets = n(),
            avg_retweets = exp(mean(log(retweets + 1))) - 1)

twt_retwt_summary %>%
  ggplot(aes(formatted_date, tweets)) +
  geom_line(color = "darkblue", size = 0.5) +
  expand_limits(y = 0) +
  labs(x = "Time",
       y = "Number of depression tweets per day")

twt_retwt_summary %>%
  ggplot(aes(formatted_date, avg_retweets)) +
  geom_line(color = "darkblue", size = 0.5) +
  expand_limits(y = 0) +
  labs(x = "Time",
       y = "Average (geometric mean) retweets each day")
```
**User Activity**
```{r}
mental %>%
  count(user_id, sort = TRUE) %>%
  head(12) %>%
  mutate(username = reorder(user_id, n)) %>%
  ggplot(aes(username, n)) +
  geom_col() +
  coord_flip()

#Retweet activity per user
mental %>%
  arrange(desc(retweets)) %>%
  select(user_id, post_text, retweets)
#Tweets und Retweets per User
mental %>%
  group_by(user_id) %>%
  summarize(tweets = n(),
            retweetnr = sum(retweets)) %>%
  arrange(desc(tweets)) %>%
  arrange(desc(retweetnr))

#Proportion Likes and Retweets per User
mental %>%
  select(user_id, post_text, retweets, favourites) %>%
  mutate(ratio = (favourites + 1) / (retweets + 1)) %>% # to avoid zero
  arrange(desc(ratio))

```

#### 8. Basics Text Mining in a Nutshell
A.**Document** = collection of strings and their metadata

B.**Corpus** = collection of *documents**. 

C.**Tokens** = smallest unit of meaning (mostly words)

D.**Vocabulary** = collection of unique words of a *Corpus*.

E.**D**ocument-**F**eature-**M**atrix or **D**ocument-**T**erm-**M**atrix = 

this is a matrix where the rows indicate the documents, the columns indicate the expressions and the cells indicate the frequency of the expressions in each document, i.e.: Matrix with *n* = number of Document rows and *m* = size of Vocabulary columns, based on **bag-of-words** assumption, which ignores word order and syntax. 



##### 8.1. Pre-processing
A. **Tokenisation**

B. **Remove **Stopwords

C. **String** operations (punctuation, normalise URL).

The order of the application steps is crucial and should be guided by the research question.

```{r Pre-processing Punctuation}
# Clean @-Symbol
mental$post_text<- gsub("@\\w+", "", mental$post_text)

# Clean Interpunction
mental$post_text<- gsub("[[:punct:]]", "", mental$post_text)

# Clean Numbers
mental$post_text <- gsub("[[:digit:]]", "", mental$post_text)

# Clean pictures
mental$post_text<- gsub("pictwitter\\w+ *", "", mental$post_text)
```

```{r Pre-processing html}
#Bereinigen von HTML-Notation
remove_html <- "&amp;|&lt;|&gt;" #&lt und &gt stehen für < und > und &amp für &

mental_en <- mental %>% 
  select(post_id, post_text, user_id) %>% 
  mutate(text = stringr::str_remove_all(post_text, remove_html))
```

With [`textclean`](https://github.com/trinker/textclean) further harmonisation can be done, e.g. replacing emojis with word equivalents.

```{r, warning=FALSE}
mental_en$text <- replace_emoji(mental_en$text)
```
Oder Normalisieren von Wortverlängerungen (Elongation, z.B."Whyyyy")
```{r, warning=FALSE}
mental_en$text <- replace_word_elongation(mental_en$text)
```

From the data frame, you can create a *Corpus* object, i.e. a collection of *documents* (tweets) and their *metadata*.

```{r}
#select unique tweet_ids

unique <- mental_en %>% group_by(post_id) %>% unique()
```



```{r Corpus Objekt, results='hide'}
tweets_en_corpus <- corpus(unique,
                           docid_field = "post_id",
                           text_field = "post_text")

```
In `quanteda` kann man Text in die kleinsten Bedeutungssegmente (**tokens**) aufteilen durch das Entfernen von Trennzeichen (vgl. dazu in `tidytext` `unnest_tokens(word, text)`. Zudem entfernen wir bestimmte Zeichentypen sowie sogenannte **Stopwords** (d.h., grammatikalische Wörter, die je nach Anwendung wenig semantische Bedeutung tragen). Vgl. (`stopwords()`)

```{r Tokenisation, results=FALSE}
tweets_en_tokens <- tokens(tweets_en_corpus,
                           remove_punct = TRUE,
                           remove_numbers = TRUE,
                           remove_symbols = TRUE,
                           remove_url = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english"))
```

```{r KWIC, results='hide'}

kw_depression <- kwic(tweets_en_tokens, pattern =  "depress*", window = 3)
head(kw_depression)
```



#### 8.2. Tidy Workflow--Exploration Tweets, Retweets und Hashtags
```{r Tokenisation und Pre-processing}
#Tokenisation der Tweets (1-gram) und Pre-processing mit tidytext

tweet_words <- mental %>%
  select(user_id, 
         post_text, 
         retweets, 
         favourites,
         formatted_date, 
         post_id) %>%
  unnest_tokens(word, post_text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% c("amp", "de", "twitter", "youre", "en", "die", "rt", "im", "dont", "ive", "yong"),
         stringr::str_detect(word, "[a-z]"))


tweet_words %>%
  count(word, sort = TRUE) %>%
  head(16) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot2::ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Most common words in Tweets on Depression",
       y = "Frequency of words")

```

```{r Favorites und Retweets, warning=FALSE}
#Verteilung Favorites
mental %>%
  ggplot2::ggplot(aes(favourites + 1)) +
  geom_histogram() +
  scale_x_log10()

#Durchschnittliche Retweets und Durschnittliche Favorites
word_summary <- tweet_words  %>%
  group_by(word) %>%
  summarize(n = n(),
            avg_retweets = exp(mean(log(retweets + 1))) - 1,
            avg_favorites = exp(mean(log(favourites + 1))) - 1) %>%
  filter(n >= 30) %>%
  arrange(desc(avg_retweets))

```


```{r tf-idf}
#Tf-idf: die Kennzahl tf-idf gibt an, wie wichtig ein Wort für ein Dokument in einer Sammlung (oder einem Korpus) von Dokumenten ist
#Häufigkeit eines Begriffs, bereinigt darum, wie selten er verwendet wird
top_word <- tweet_words %>%
  count(word, formatted_date) %>%
  bind_tf_idf(word, formatted_date, n) %>%
  arrange(desc(tf_idf)) %>%
  distinct(formatted_date, .keep_all = TRUE) %>%
  arrange(formatted_date)

tf_idf <-word_summary %>%
  inner_join(top_word, by = c("word")) %>%
  arrange(desc(avg_retweets)) 

#Tf-idf der Wörter pro Stunde
tf_idf %>% 
  ggplot2::ggplot(aes(tf_idf, forcats::fct_reorder(word, tf_idf), fill = formatted_date)) +
  geom_col(show.legend = FALSE)
```

```{r hashtags, warning=FALSE}
#Hashtag Exploration
tweet_words <- mental %>%
  mutate(hashtags = stringr::str_count(post_text, "#[a-zA-Z]"), sort = TRUE) %>%
  select(user_id, post_text, retweets, favourites, formatted_date , post_id,
        hashtags) %>%
  unnest_tokens(word, post_text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% c("de", "|"),
         stringr::str_detect(word, "[a-z]"))


mental %>%
  mutate(hashtags = stringr::str_count(post_text, "#[a-zA-Z]"), sort = TRUE) %>%
  filter(hashtags < 6) %>%
  group_by(user_id) %>%
  summarize(tweets = n(),
            avg_retweets = exp(mean(log(retweets + 1))) - 1) %>%
  filter(tweets >= 30) %>%
  arrange(desc(avg_retweets))


tweet_word_summary <- tweet_words %>%
  filter(hashtags < 6) %>%
  group_by(word) %>%
  summarize(n = n(),
            avg_retweets = exp(mean(log(retweets+ 1))) - 1,
            avg_favorites = exp(mean(log(favourites + 1))) - 1)



tweet_word_summary <-tweet_word_summary %>%
  filter(n >= 100,
         !stringr::str_detect(word, "https")) %>%
   filter(!word %in% c("amp", "de", "twitter", "youre", "en", "die", "rt", "im", "dont", "ive", "yong"),
         stringr::str_detect(word, "[a-z]")) %>% 
  arrange(desc(avg_retweets)) %>% 
  head()

tweet_word_summary %>%
  filter(n >= 100,
         !stringr::str_detect(word, "https")) %>%
  ggplot2::ggplot(aes(n, avg_retweets)) +
  geom_point() +
  geom_text(aes(label = word), check_overlap = TRUE) +
  scale_x_log10() +
  scale_y_log10()

tweet_word_summary %>%
  filter(n >= 100,
         !stringr::str_detect(word, "https")) %>%
  arrange(desc(avg_retweets)) %>%
  head(20) %>%
  mutate(word = reorder(word, avg_retweets)) %>%
  ggplot2::ggplot(aes(word, avg_retweets)) +
  geom_col() +
  coord_flip() +
  labs(title = "Which words get the most retweets in depression?",
       subtitle = "Only words appearing in at least 100 tweets",
       y = "Geometric mean of the Number of retweets")
```


#### 9. Daten speichern
```{r, eval=FALSE}
write_csv(tweets, file = "./data/tweets.csv")
```


#### Ausblick und Quellen

Barrie, Christopher & Ho, Justin Chun-ting. (2021). academictwitteR: an R package to access the Twitter Academic Research Product Track v2 API endpoint. *Journal of Open Source Software*, *6*(62), 3272, https://doi.org/10.21105/joss.03272

[Breuer, J. (2022). Demo Twitter-Daten mit R](https://github.com/jobreu/demo-twitter-r)

[Breuer, J. (2022). Twitter linking workshop](https://github.com/jobreu/twitter-linking-workshop-2022)

[Breuer, J., Kohne, J., &  Mohseni, M.R. (2021). Workshop "Automatic Sampling and Analysis of YouTube Comments", GESIS 2021](https://github.com/jobreu/youtube-workshop-gesis-2021)

[Hvitfeldt, E. & Silge, J. (2022). Supervised Machine Learning for Text Analysis in R von ](https://smltar.com/)

[Silge, J. & Robinson, D. (2022). Text Mining with R - A Tidy Approach von ](https://www.tidytextmining.com/)

[Niekler, A. & Wiedemann, G. (2020). Text mining in R for the social sciences and digital humanities von A](https://tm4ss.github.io/docs/)

[Twitter datasets](https://github.com/shaypal5/awesome-twitter-data)
[MetaCorpus of social media corpora](https://github.com/socialmediaie/MetaCorpus)

[Twitter datasets](https://vlo.clarin.eu/search;jsessionid=1AAE84B9F01F93D93D755DAEF7BCE0FF?0&q=twitter)



