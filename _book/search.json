[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Text mining with Twitter data",
    "section": "",
    "text": "Preface\nThis workshop will include an introduction to how large amounts of text data from Twitter, which are openly available, can be made accessible and usable for research purposes. It will combine conceptual considerations and practical applications in R.\n\nStrategies to collect and process textual data with application programming interfaces (APIs) using common R tools.\nPotentials of bias in the research data cycle\nBasics of natural language processing (NLP), data cleaning (e.g. with quanteda or textclean) and application of common NLP tools for automated text analysis\nOutlook on topic modelling (or word embeddings)\nBias and ethics in NLP\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[Text mining with Twitter data]{.hidden render-id=\"quarto-int-sidebar-title\"}\n[Text mining with Twitter data]{.hidden render-id=\"quarto-int-navbar-title\"}\n[&lt;span class='chapter-number'&gt;1&lt;/span&gt;  &lt;span class='chapter-title'&gt;Introduction&lt;/span&gt;]{.hidden render-id=\"quarto-int-next\"}\n[Preface]{.hidden render-id=\"quarto-int-sidebar:/index.html\"}\n[&lt;span class='chapter-number'&gt;1&lt;/span&gt;  &lt;span class='chapter-title'&gt;Introduction&lt;/span&gt;]{.hidden render-id=\"quarto-int-sidebar:/intro.html\"}\n[&lt;span class='chapter-number'&gt;2&lt;/span&gt;  &lt;span class='chapter-title'&gt;Use case Twitter text mining&lt;/span&gt;]{.hidden render-id=\"quarto-int-sidebar:/summary.html\"}\n[References]{.hidden render-id=\"quarto-int-sidebar:/references.html\"}\n[Preface]{.hidden render-id=\"quarto-breadcrumbs-5cf06822087ff10dec2ac74cf1e20d30\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[Text mining with Twitter data]{.hidden render-id=\"quarto-metatitle\"}\n[Text mining with Twitter data]{.hidden render-id=\"quarto-twittercardtitle\"}\n[Text mining with Twitter data]{.hidden render-id=\"quarto-ogcardtitle\"}\n[Text mining with Twitter data]{.hidden render-id=\"quarto-metasitename\"}\n[]{.hidden render-id=\"quarto-twittercarddesc\"}\n[]{.hidden render-id=\"quarto-ogcardddesc\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n# Preface {.unnumbered}\n\nThis workshop will include an introduction to how large amounts of text data from Twitter, which are openly available, can be made accessible and usable for research purposes. It will combine conceptual considerations and practical applications in R.\n\n- Strategies to collect and process textual data with application programming interfaces (APIs) using common `R` tools.\n\n- Potentials of bias in the research data cycle\n\n- Basics of natural language processing (NLP), data cleaning (e.g. with `quanteda` or `textclean`) and application of common NLP tools for automated text analysis\n\n- Outlook on topic modelling (or word embeddings)\n\n- Bias and ethics in NLP\n\n\n\n``````````````````` :::"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "1. Basic workflow for working with text data\nA) Sampling Text Data\n\n\nExisting Archives (e.g., Reddit Pushift Data, see also Baumgartner et al., 2020)\nAPIs (e.g., The Guardian)\nWeb Scraping (e.g., with rvest) (or just for fun, build you own API with plumber)\nB) Pre-processing/ Data Wrangling\n\n\nCleaning\nSelecting and Weightning of Features (Reducing dimensionality)\nC) Analyses\n\n\n\nDictionary- or rulebased Approaches\nSupervised Approaches (e.g., SVM)\nUnsupervised Approaches (e.g., Topic Modeling)\nD) Validation\n\n\ne.g., Reliability of Bot Classifications (tweetbotornot2)\n2. Data Sampling\nA) Screen Scraping:\n\n\nScraping, Parsing and Formatting (e.g., with Rselenium)\nB) API Access Points:\n\nSend Get-Requests directly to the data base\nGateways for specific data types, irrespective of the programming language\nB.1. Application Programming Interfaces (APIs)\n\n“communicates” directly with the data base\ndetermines which information are accessible for whom, how and to which degree\n\nB.2. API-Applications\n\nEmbedded content into other applications\nBuild bots (e.g., Telegram)\nCollect data for market research\n\nAccess point exist for:\n\nYoutube – allows via keywords to search for contents, the Video, lists and user activities such as upvoting, comments, favorites\nInstagram – allows to search for comment structures relating to postings, friends information of users or geolocation\nWikipedia – allows to search for MediaWiki revisions, revision summaries connected to an entry, timestamps, site information, user information\nGoogle Maps – allows to search for coordinates of latitude and longitude, distance matrices\nB.3. Advantages of API harvesting\n\nNo interaction with HTML data types necessary (Output: JSON-files)\nUsually legal (upon following the Terms of Service)\nB.4. Disadvantages of API harvesting\n\nNot every website has an API\nOnly the data that the API makes available can be retrieved\n\nRate limitations (e.g. number of tweets per day/ query).\nTerms of use and changes to the API restrict use (e.g. code reproducibility, sharing of data sets)\nCode varies depending on platform and level of documentation detail\n3. Tool overview for Twitter data access\nTwitter for research on the dynamics of fast-moving socio-political events and contemporary culture.\nDifferentiation by requirements:\n\nGraphical User Interface\nType of data\nSuitability for collection or for processing and analysis\nAPI version (e.g. Twitter API v1.1 offer the packages rtweet)\n\nSources for Tools:\n\nTwitter Tool List: Wiki of the Social Media Observatory am Leibniz-Institut für Medienforschung | Hans-Bredow-Institut (HBI)\nTutorial for Twitter-Daten: Jürgens, P., & Jungherr, A. (2016). A tutorial for using Twitter data in the social sciences: Data collection, preparation, and analysis. Preparation, and Analysis (January 5, 2016).)\nTools for Collecting and Analysing Social Media Data\n4. Academic Twitter Access Point (v2 API endpoints)\n\nTo run the following code examples, Academic Research Access for the Twitter API v2 is required. To sample Twitter data, one uses the R package academictwitteR.\nIn the framework, one needs the so-called Bearer Token. This vignette in the academictwitteR package explains the process of gaining access to the Twitter API.\n4.1. Twitter Modi with v2 API Endpoints\n\n# Tweet-centric sampling (samples 1000 tweets)\n\nget_all_tweets(\n  query = 'xyz',\n  start_tweets,\n  end_tweets,\n  bearer_token = get_bearer(),\n  n = 1000\n)\n\n\n# User-centric sampling: defining users\n\nusers &lt;- c(\"juliasilge\", \"drob\")\n\nget_user_id(users, bearer_token)\n\n\n# User-centric sampling (samples 100 tweets)\n\nget_user_timeline(\nuserid, #single string or Vector with User_Ids\nstart_tweets,\nend_tweets,\nbearer_token = get_bearer(),\nn = 100,\n)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barrie, Christopher & Ho, Justin Chun-ting. (2021). academictwitteR: an R package to access the Twitter Academic Research Product Track v2 API endpoint. Journal of Open Source Software, 6(62), 3272, https://doi.org/10.21105/joss.03272\nBreuer, J. (2022). Demo Twitter-Daten mit R\nBreuer, J. (2022). Twitter linking workshop\nBreuer, J., Kohne, J., & Mohseni, M.R. (2021). Workshop “Automatic Sampling and Analysis of YouTube Comments”, GESIS 2021\nHvitfeldt, E. & Silge, J. (2022). Supervised Machine Learning for Text Analysis in R von\nJürgens, P., & Jungherr, A. (2016). A tutorial for using Twitter data in the social sciences: Data collection, preparation, and analysis. Preparation, and Analysis (January 5, 2016).)\nSilge, J. & Robinson, D. (2022). Text Mining with R - A Tidy Approach von\nNiekler, A. & Wiedemann, G. (2020). Text mining in R for the social sciences and digital humanities von A\nTwitter datasets MetaCorpus of social media corpora\nTwitter datasets"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "\n2  Use case Twitter text mining\n",
    "section": "",
    "text": "Required Rpackages\n\npkgs &lt;- c(\"kaggler\", \"tidyverse\", \"httr\", \"readr\", \"here\") \n\nlapply(pkgs, require, character.only = TRUE)\n#else install kaggler:\n#install.packages(c(\"devtools\"))\n#devtools::install_github(\"ldurazo/kaggler\")\n\nfilter &lt;- dplyr::filter # to resolve namespace conflicts between tidyverse and dplyr\n\nlibrary(academictwitteR)    # collecting Twitter data\nlibrary(dplyr)              # Data Wrangling, pre-processing\nlibrary(quanteda)           # text-based descriptive\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(tidytext)           # Text Mining 'tidy format'\nlibrary(textclean)          # Pre-Processing + harmonizing\nlibrary(lubridate)          # Wrangling with time series data\nlibrary(ggplot2)\nlibrary(anytime)            # parsing dates\nlibrary(scales)\nlibrary(caret)\n\ntheme_set(theme_light())\n\nDescriptive Overview\nFor demonstration purposes we retrieve an archived Twitter (sub-)sample, containing English depression specific tweets. (The sample contains N = 20,000 tweets.) We retrieve a sub-sample from the platform Kaggle.\na.) This can be directly downloaded: Kaggle Data Dump, Depression”\nb.) Or by issuing a GET request via its API (see below).\nFor obtaining the data via the Kaggle API we install and load all dependencies to use kaggler.\nNote. For both (a & b authentication is required)\nA kaggle.json file needs to be made for authentication on Kaggle. The kaggle.json file should be positioned within the working directory. The json contains: username and the API token.\nTo create your API token go to your Kaggle Profil: (Your profile &gt; Account &gt; Settings &gt; API &gt; Create new token)\n\nsetwd(\"C:/Users/batzdova/Desktop\")\nlibrary(here)\nhere::i_am(\"kaggle.json\")\nkgl_auth(creds_file = 'kaggle.json')\n\n\nresponse &lt;- kgl_datasets_download_all(owner_dataset = \"infamouscoder/mental-health-social-media?select=Mental-Health-Twitter\")\n\n\nlibrary(httr)\ndataset &lt;- httr::GET(\"https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv\")\n\ntemp &lt;- tempfile()\ndownload.file(dataset$url,temp)\ndata &lt;- read.csv(unz(temp, \"Mental-Health-Twitter.csv\"))\nunlink(temp)\n\n\nmental &lt;- data\n\nOr alternatively, read in csv-data which have been downloaded.\n\n2.0.1 Overview of observations\nOur data set comprises \\(20,000\\) observations with \\(10,000\\) depression labeled tweets. Yet, only \\(19,881\\) are unique tweets (i.e. post_id)\nEach observed person user_id (\\(N=72\\) ) can be characterized by a set of variables (i.e., features) such as date of post creating post_created or count of followers.\n\nglimpse(mental)\n\nRows: 20,000\nColumns: 11\n$ ...1         &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,~\n$ post_id      &lt;dbl&gt; 6.378947e+17, 6.378904e+17, 6.377493e+17, 6.3~\n$ post_created &lt;chr&gt; \"Sun Aug 30 07:48:37 +0000 2015\", \"Sun Aug 30~\n$ post_text    &lt;chr&gt; \"It's just over 2 years since I was diagnosed~\n$ user_id      &lt;dbl&gt; 1013187241, 1013187241, 1013187241, 101318724~\n$ followers    &lt;dbl&gt; 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8~\n$ friends      &lt;dbl&gt; 211, 211, 211, 211, 211, 211, 211, 211, 211, ~\n$ favourites   &lt;dbl&gt; 251, 251, 251, 251, 251, 251, 251, 251, 251, ~\n$ statuses     &lt;dbl&gt; 837, 837, 837, 837, 837, 837, 837, 837, 837, ~\n$ retweets     &lt;dbl&gt; 0, 1, 0, 2, 1, 1, 1, 0, 0, 41, 1, 0, 0, 0, 0,~\n$ label        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n\nmental %&gt;% \n  filter(label ==1) %&gt;% \n  count()\n\n# A tibble: 1 x 1\n      n\n  &lt;int&gt;\n1 10000\n\nn_distinct(mental$post_id) # count of unique tweets\n\n[1] 19881\n\nn_distinct(mental$user_id) # count of unique users\n\n[1] 72\n\n\n\nmental %&gt;%\n  mutate(no_mentions = str_remove_all(string = post_text, pattern = \"[@][\\\\w_-]+\")) %&gt;% \n  dplyr::select(post_text, no_mentions) %&gt;% \n  View()\n\nParse Date Variable post_created\n\n# Remove the timezone offset from the date string\nmental$date &lt;- gsub(\" \\\\+\\\\d{4}\", \"\", mental$post_created)\n\n\n# Convert the date string to a date variable\nmental$date_any &lt;- anytime(mental$date)\n\n# Format the date variable as \"YYYY-MM-DD H:M\"\n#mental$formatted_date &lt;- format(mental$date_any , \"%Y-%m-%d %H:%M\")\nmental$formatted_date &lt;- format(mental$date_any , \"%Y-%m-%d\")\nmental$day_hour &lt;- format(mental$date_any, \"%Y-%m-%d %H\")\n\nFormatted_date is a character class. When we would use as.Date() we would lose the timestamp information. When we have a character with both date and time we have to use POSIXct class (as seen with the date_any variable).\n\nmental$date &lt;- as.Date(mental$formatted_date)\nmental$date_hour &lt;- as.POSIXlt(mental$day_hour)\n#mental$formatted_date_time &lt;- as.POSIXlt(mental$formatted_date)\n\n\nmental %&gt;%\n\n# calculate how many tweets posted on each date\n  count(date) %&gt;%\n\n# plot the result\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  theme_bw() +\n  scale_x_date(date_breaks = \"7 day\", date_labels = \"%b %d\") +\n  ylab(\"Number of Tweets\\n\") + xlab(\"\\nDate\")\n\n\n\n\nDistribution Friends\n\nmental$logfriends &lt;-log(mental$friends+1)\n\nhist(mental$logfriends,  breaks = 50, prob= TRUE)\nlines(density(mental$logfriends), col = \"red\")  \n\n\n\n\nDistribution Followers\n\nmental$logfollow=log(mental$followers+1)\nhist(mental$logfollow, breaks = 50, prob = TRUE)\nlines(density(mental$logfollow), col = \"red\")   \n\n\n\n\n\nggplot(mental, aes(x = logfriends, y = logfollow)) + \n  geom_point(alpha=.5)\n\n\n\n\nTweets und Retweets per day\n\ntwt_retwt_summary &lt;- mental %&gt;%\n  group_by(user_id, date, label) %&gt;%\n  summarize(tweets = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1)\n\nstartTime &lt;- as.Date(\"2014-01-01\")\nendTime &lt;- as.Date(\"2017-10-31\")\n\n# create a start and end time R object\nstart.end &lt;- c(startTime,endTime)\nstart.end\n\n[1] \"2014-01-01\" \"2017-10-31\"\n\npl1 &lt;- twt_retwt_summary %&gt;%\n  ggplot(aes(date, tweets, group = user_id, fill= user_id)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\n\ntwt_retwt_summary %&gt;%\n  ggplot(aes(date, tweets, group = user_id, fill= label, col= label)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\n\n\ntwt_retwt_summary %&gt;%\n  ggplot(aes(date, avg_retweets, group = user_id, fill= label, col= label)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\n\n\npl2 &lt;-twt_retwt_summary %&gt;%\n  ggplot(aes(date, avg_retweets, group=user_id)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Average (geometric mean) retweets each day\")+\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\ngridExtra::grid.arrange(pl1,pl2, ncol=2)\n\n\n\n\n\nts.plot(twt_retwt_summary$tweets)\n\n\n\nts.plot(twt_retwt_summary$avg_retweets)\n\n\n\n\nUser Activity\n\nmental %&gt;%\n  count(user_id, sort = TRUE) %&gt;%\n  head(12) %&gt;%\n  mutate(username = reorder(user_id, n)) %&gt;%\n  ggplot(aes(username, n)) +\n  geom_col() +\n  coord_flip()\n\n\n\n#Retweet activity per user\nmental %&gt;%\n  arrange(desc(retweets)) %&gt;%\n  select(user_id, post_text, retweets)\n\n# A tibble: 20,000 x 3\n      user_id post_text                                     retweets\n        &lt;dbl&gt; &lt;chr&gt;                                            &lt;dbl&gt;\n 1  490044008 RT @omgAdamSaleh: We got kicked out of a @De~   839540\n 2 3249600438 RT @POTUS: Thank you for everything. My last~   822450\n 3 1497350173 RT @POTUS: Thank you for everything. My last~   820245\n 4 1306425758 RT @Louis_Tomlinson: Our fans support really~   402946\n 5 1497350173 RT @HamillHimself: no words #Devastated http~   358548\n 6 3249600438 RT @HamillHimself: no words #Devastated http~   358536\n 7  171999132 RT @HamillHimself: no words #Devastated http~   358510\n 8 1306425758 RT @Louis_Tomlinson: @NaughtyBoyMusic Jesus ~   341695\n 9 1306425758 RT @Louis_Tomlinson: @NaughtyBoyMusic good f~   306473\n10  490044008 RT @edsheeran: Hello 2017... https://t.co/1U~   273872\n# i 19,990 more rows\n\n#Tweets und Retweets per User\nmental %&gt;%\n  group_by(user_id) %&gt;%\n  summarize(tweets = n(),\n            retweetnr = sum(retweets)) %&gt;%\n  arrange(desc(tweets)) %&gt;%\n  arrange(desc(retweetnr))\n\n# A tibble: 72 x 3\n      user_id tweets retweetnr\n        &lt;dbl&gt;  &lt;int&gt;     &lt;dbl&gt;\n 1 1497350173    792   7004039\n 2  490044008   2117   5379581\n 3 1306425758    306   4810463\n 4 3045320288    633   4402064\n 5 3249600438   1504   4131286\n 6  324294391    215    765268\n 7  171999132    121    463537\n 8   18831261    543    317468\n 9   49548465    352    197019\n10 1015648590     84    175150\n# i 62 more rows\n\n#Proportion Likes and Retweets per User\nmental %&gt;%\n  select(user_id, post_text, retweets, favourites) %&gt;%\n  mutate(ratio = (favourites + 1) / (retweets + 1)) %&gt;% # to avoid zero\n  arrange(desc(ratio))\n\n# A tibble: 20,000 x 5\n      user_id post_text                    retweets favourites ratio\n        &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 2660477449 \"@BenBuckwalter i was diagn~        0      39008 39009\n 2 2660477449 \"@JordanSweeto i hope whate~        0      39008 39009\n 3 2660477449 \"@BenBuckwalter i would!\"           0      39008 39009\n 4 2660477449 \"@BenBuckwalter i feel like~        0      39008 39009\n 5 2660477449 \"@BryanStars I love you,  y~        0      39008 39009\n 6 2660477449 \"@BenBuckwalter me! I volun~        0      39008 39009\n 7 2660477449 \"I'm getting tired of your ~        0      39008 39009\n 8 2660477449 \"@deefizzy you're amazing. ~        0      39008 39009\n 9 2660477449 \"@BenBuckwalter people tend~        0      39008 39009\n10 2660477449 \"@BenBuckwalter being notic~        0      39008 39009\n# i 19,990 more rows\n\n\n8. Basics Text Mining in a Nutshell\nA.Document = collection of strings and their metadata\nB.Corpus = collection of *documents**.\nC.Tokens = smallest unit of meaning (mostly words)\nD.Vocabulary = collection of unique words of a Corpus.\nE.Document-Feature-Matrix or Document-Term-Matrix =\nthis is a matrix where the rows indicate the documents, the columns indicate the expressions and the cells indicate the frequency of the expressions in each document, i.e.: Matrix with n = number of Document rows and m = size of Vocabulary columns, based on bag-of-words assumption, which ignores word order and syntax.\n8.1. Pre-processing\nA. Tokenisation\nB. Remove Stopwords\nC. String operations (punctuation, normalise URL).\nThe order of the application steps is crucial and should be guided by the research question.\n\n# Clean @-Symbol\nmental$post_text&lt;- gsub(\"@\\\\w+\", \"\", mental$post_text)\n\n# Clean Interpunction\nmental$post_text&lt;- gsub(\"[[:punct:]]\", \"\", mental$post_text)\n\n# Clean Numbers\nmental$post_text &lt;- gsub(\"[[:digit:]]\", \"\", mental$post_text)\n\n# Clean pictures\nmental$post_text&lt;- gsub(\"pictwitter\\\\w+ *\", \"\", mental$post_text)\n\n\n#Clean HTML-Notation\nremove_html &lt;- \"&amp;|&lt;|&gt;\" #&lt und &gt stehen für &lt; und &gt; und &amp für & &\n\nmental_en &lt;- mental %&gt;% \n  select(post_id, post_text, user_id) %&gt;% \n  mutate(text = stringr::str_remove_all(post_text, remove_html))\n\nWith textclean further harmonisation can be done, e.g. replacing emojis with word equivalents.\n\nmental_en$text &lt;- replace_emoji(mental_en$text)\n\nOr normalizing colloquial expressions such as word elongations (e.g., “Whyyyy”)\n\nmental_en$text &lt;- replace_word_elongation(mental_en$text)\n\nFrom the data frame, you can create a Corpus object, i.e. a collection of documents (tweets) and their metadata.\n\n#select unique tweet_ids\nunique &lt;- mental_en %&gt;% group_by(post_id) %&gt;% unique()\n\n\ntweets_en_corpus &lt;- corpus(unique,\n                           docid_field = \"post_id\",\n                           text_field = \"post_text\")\n\nIn quanteda the smallest meaningful unit are tokens which can be generated by segmenting into words (see tidytext unnest_tokens(word, text). Further, we can remove stopwords (hedges, grammatical signifiers which bear little semantic meaning). See (stopwords())\n\ntweets_en_tokens &lt;- tokens(tweets_en_corpus,\n                           remove_punct = TRUE,\n                           remove_numbers = TRUE,\n                           remove_symbols = TRUE,\n                           remove_url = TRUE) %&gt;% \n  tokens_tolower() %&gt;% \n  tokens_remove(stopwords(\"english\"))\n\n\nkw_depression &lt;- kwic(tweets_en_tokens, pattern =  \"depress*\", window = 3)\nhead(kw_depression)\n\n8.2. Tidy Workflow–Exploration Tweets, Retweets and Hashtags\n\n#Tokenising Tweets (1-gram) and Pre-processing with tidytext\n\ntweet_words &lt;- mental %&gt;%\n  select(user_id, \n         post_text, \n         retweets, \n         favourites,\n         formatted_date, \n         post_id) %&gt;%\n  unnest_tokens(word, post_text) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"amp\", \"de\", \"twitter\", \"youre\", \"en\", \"die\", \"rt\", \"im\", \"dont\", \"ive\", \"yong\"),\n         stringr::str_detect(word, \"[a-z]\"))\n\ntweet_words %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  head(16) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot2::ggplot(aes(word, n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Most common words in Tweets on Depression\",\n       y = \"Frequency of words\")\n\n\n\n\n\n#Distribution of Favorites\nmental %&gt;%\n  ggplot2::ggplot(aes(favourites + 1)) +\n  geom_histogram() +\n  scale_x_log10()\n\n\n\n#Mean Retweets and Mean Favorites\nword_summary &lt;- tweet_words  %&gt;%\n  group_by(word) %&gt;%\n  summarize(n = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1,\n            avg_favorites = exp(mean(log(favourites + 1))) - 1) %&gt;%\n  filter(n &gt;= 30) %&gt;%\n  arrange(desc(avg_retweets))\n\n\n#Tf-idf: signals importance of a word in a corpus selection\n\ntop_word &lt;- tweet_words %&gt;%\n  count(word, formatted_date) %&gt;%\n  bind_tf_idf(word, formatted_date, n) %&gt;%\n  arrange(desc(tf_idf)) %&gt;%\n  distinct(formatted_date, .keep_all = TRUE) %&gt;%\n  arrange(formatted_date)\n\ntf_idf &lt;- word_summary %&gt;%\n  inner_join(top_word, by = c(\"word\")) %&gt;%\n  arrange(desc(avg_retweets)) \n\n#Tf-idf of words per hour\ntf_idf %&gt;% \n  ggplot2::ggplot(aes(tf_idf, forcats::fct_reorder(word, tf_idf), fill = word)) +\n  geom_col(show.legend = FALSE)\n\n\n\n\n\n#Hashtag exploration\ntweet_words &lt;- mental %&gt;%\n  mutate(hashtags = stringr::str_count(post_text, \"#[a-zA-Z]\"), sort = TRUE) %&gt;%\n  select(user_id, post_text, retweets, favourites, formatted_date , post_id,\n        hashtags) %&gt;%\n  unnest_tokens(word, post_text) %&gt;%\n  anti_join(stop_words, by = \"word\") %&gt;%\n  filter(!word %in% c(\"de\", \"|\"),\n         stringr::str_detect(word, \"[a-z]\"))\n\n\nmental %&gt;%\n  mutate(hashtags = stringr::str_count(post_text, \"#[a-zA-Z]\"), sort = TRUE) %&gt;%\n  filter(hashtags &lt; 6) %&gt;%\n  group_by(user_id) %&gt;%\n  summarize(tweets = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1) %&gt;%\n  filter(tweets &gt;= 30) %&gt;%\n  arrange(desc(avg_retweets))\n\n# A tibble: 55 x 3\n   user_id tweets avg_retweets\n     &lt;dbl&gt;  &lt;int&gt;        &lt;dbl&gt;\n 1 3.05e 9    633       229.  \n 2 1.50e 9    792        74.8 \n 3 7.40e 7     50        72.4 \n 4 1.31e 9    306        59.8 \n 5 1.72e 8    121        11.2 \n 6 3.09e 7    142        11.1 \n 7 3.25e 9   1504         6.85\n 8 7.63e17    522         6.79\n 9 6.32e 8    144         5.09\n10 4.95e 7    352         4.26\n# i 45 more rows\n\ntweet_word_summary &lt;- tweet_words %&gt;%\n  filter(hashtags &lt; 6) %&gt;%\n  group_by(word) %&gt;%\n  summarize(n = n(),\n            avg_retweets = exp(mean(log(retweets+ 1))) - 1,\n            avg_favorites = exp(mean(log(favourites + 1))) - 1)\n\n\n\ntweet_word_summary &lt;-tweet_word_summary %&gt;%\n  filter(n &gt;= 100,\n         !stringr::str_detect(word, \"https\")) %&gt;%\n   filter(!word %in% c(\"amp\", \"de\", \"twitter\", \"youre\", \"en\", \"die\", \"rt\", \"im\", \"dont\", \"ive\", \"yong\"),\n         stringr::str_detect(word, \"[a-z]\")) %&gt;% \n  arrange(desc(avg_retweets)) %&gt;% \n  head()\n\ntweet_word_summary %&gt;%\n  filter(n &gt;= 100,\n         !stringr::str_detect(word, \"https\")) %&gt;%\n  ggplot2::ggplot(aes(n, avg_retweets)) +\n  geom_point() +\n  geom_text(aes(label = word), check_overlap = TRUE) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\ntweet_word_summary %&gt;%\n  filter(n &gt;= 100,\n         !stringr::str_detect(word, \"https\")) %&gt;%\n  arrange(desc(avg_retweets)) %&gt;%\n  head(20) %&gt;%\n  mutate(word = reorder(word, avg_retweets)) %&gt;%\n  ggplot2::ggplot(aes(word, avg_retweets)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Which words get the most retweets in depression?\",\n       subtitle = \"Only words appearing in at least 100 tweets\",\n       y = \"Geometric mean of the Number of retweets\")\n\n\n\n\n\n3 Predicting depression tweets based on followers and friends\n\n# Data preprocessing\n# Check for missing values\nmissing_values &lt;- sum(is.na(mental))\nif (missing_values &gt; 0) {\n  data &lt;- na.omit(mental)  # Remove rows with missing values\n}\n\n# Convert label variable to factor\n#mental$label &lt;- as.factor(mental$label)\n\n# Standardize the followers and friends variables\nmental$followers_standardized &lt;- as.numeric(scale(mental$followers))\nmental$friends_standardized &lt;- as.numeric(scale(mental$friends))\n\n# Check the standardized variables\nhead(mental[, c(\"followers\", \"followers_standardized\", \"friends\", \"friends_standardized\")])\n\n# A tibble: 6 x 4\n  followers followers_standardized friends friends_standardized\n      &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;                &lt;dbl&gt;\n1        84                 -0.430     211               -0.311\n2        84                 -0.430     211               -0.311\n3        84                 -0.430     211               -0.311\n4        84                 -0.430     211               -0.311\n5        84                 -0.430     211               -0.311\n6        84                 -0.430     211               -0.311\n\n# Select variables for analysis\nselected_vars &lt;- c(\"label\", \"followers_standardized\", \"friends_standardized\")\nmental_selected &lt;- mental[selected_vars]\n\n\n# Split the dataset into training and testing sets\nset.seed(123)  # Set a seed for reproducibility\ntrain_indices &lt;- createDataPartition(mental_selected$label, p = 0.8, list = FALSE)\ntrain_data &lt;- mental_selected[train_indices, ]\ntest_data &lt;- mental_selected[-train_indices, ]\n\n\ntrain_data$label &lt;- as.numeric(train_data$label)\ntest_data$label &lt;- as.numeric(test_data$label)\n\n# Linear Regression model\nmodel &lt;- lm(label ~ followers_standardized + friends_standardized, data = train_data)\n\n# Model evaluation\npredictions &lt;- predict(model, newdata = test_data)\nevaluation &lt;- data.frame(actual = test_data$label, predicted = predictions)\n\n# Calculate evaluation metrics\nrmse &lt;- mean((evaluation$actual - evaluation$predicted)^2)\nrmse &lt;- sqrt(rmse)\nr_squared &lt;- cor(evaluation$actual, evaluation$predicted)^2\n\n# Print evaluation metrics\ncat(\"Mean Squared Error (MSE):\", rmse, \"\\n\")\n\nMean Squared Error (MSE): 0.4776674 \n\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n\nRoot Mean Squared Error (RMSE): 0.4776674 \n\ncat(\"R-squared value:\", r_squared, \"\\n\")\n\nR-squared value: 0.08754619 \n\n# Regression coefficients and p-values\nsummary(model)\n\n\nCall:\nlm(formula = label ~ followers_standardized + friends_standardized, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8265 -0.4814 -0.0809  0.4749  1.1280 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             0.499941   0.003789  131.94   &lt;2e-16 ***\nfollowers_standardized -0.286159   0.008359  -34.23   &lt;2e-16 ***\nfriends_standardized    0.314163   0.008355   37.60   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4793 on 15997 degrees of freedom\nMultiple R-squared:  0.08129,   Adjusted R-squared:  0.08117 \nF-statistic: 707.7 on 2 and 15997 DF,  p-value: &lt; 2.2e-16\n\nlibrary(dotwhisker)\ndwplot(model)+  \n   xlab(\"Coefficient Estimate\") + ylab(\"\") +\n    geom_vline(xintercept = 0,\n               colour = \"grey60\",\n               linetype = 2) +\n  ggtitle(\"Predicting Depression Tweets\") \n\n\n\n\nThe followers_standardized coefficient (-0.286159) indicates that, on average, a one-standard-deviation increase in the standardized followers variable is associated with a decrease of 0.286159 units in the predicted depression label. The friends_standardized coefficient (0.314163) indicates that, on average, a one-standard-deviation increase in the standardized friends variable is associated with an increase of 0.314163 units in the predicted depression label. All t-values are large (absolute values &gt; 2), indicating that the coefficients are significantly different from zero.\nOverall, the regression analysis suggests that both the standardized followers and standardized friends variables have a statistically significant association with the predicted depression label. An increase in the standardized followers is associated with a decrease in the predicted depression label, while an increase in the standardized friends is associated with an increase in the predicted depression label.\n\n# Fit a non-linear regression model\nmodel_nonlinear &lt;- nls(label ~ a * exp(b * followers_standardized) + c * friends_standardized,\n                       data = train_data,\n                       start = list(a = 1, b = 1, c = 1))\n\n# Print the model summary\nsummary(model_nonlinear)\n\n\nFormula: label ~ a * exp(b * followers_standardized) + c * friends_standardized\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \na  0.478974   0.004230  113.23   &lt;2e-16 ***\nb -0.510885   0.024174  -21.13   &lt;2e-16 ***\nc  0.095444   0.003802   25.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4864 on 15997 degrees of freedom\n\nNumber of iterations to convergence: 17 \nAchieved convergence tolerance: 3.114e-07\n\n# Predict the label using the non-linear regression model\ntest_data$predicted_nonlinear &lt;- predict(model_nonlinear, newdata = test_data)\n\n# Evaluate the non-linear regression model\nmse_nonlinear &lt;- mean((test_data$label - test_data$predicted_nonlinear)^2)\nrmse_nonlinear &lt;- sqrt(mse_nonlinear)\nr_squared_nonlinear &lt;- 1 - sum((test_data$label - test_data$predicted_nonlinear)^2) / sum((test_data$label - mean(test_data$label))^2)\n\n# Print evaluation metrics\ncat(\"Mean Squared Error (MSE) - Nonlinear Regression:\", mse_nonlinear, \"\\n\")\n\nMean Squared Error (MSE) - Nonlinear Regression: 0.2366316 \n\ncat(\"Root Mean Squared Error (RMSE) - Nonlinear Regression:\", rmse_nonlinear, \"\\n\")\n\nRoot Mean Squared Error (RMSE) - Nonlinear Regression: 0.4864479 \n\ncat(\"R-squared value - Nonlinear Regression:\", r_squared_nonlinear, \"\\n\")\n\nR-squared value - Nonlinear Regression: 0.05347357 \n\n\n\ndwplot(model_nonlinear)+  \n   xlab(\"Coefficient Estimate\") + ylab(\"\") +\n    geom_vline(xintercept = 0,\n               colour = \"grey60\",\n               linetype = 2) +\n  ggtitle(\"Predicting Depression Tweets with a non-linear model\") \n\n\n\n\n\nlibrary(gam)\n\nWarning: Paket 'gam' wurde unter R Version 4.1.3 erstellt\n\n\nLade nötiges Paket: splines\n\n\nLade nötiges Paket: foreach\n\n\n\nAttache Paket: 'foreach'\n\n\nDie folgenden Objekte sind maskiert von 'package:purrr':\n\n    accumulate, when\n\n\nLoaded gam 1.22\n\nlibrary(mgcv)\n\nWarning: Paket 'mgcv' wurde unter R Version 4.1.3 erstellt\n\n\nLade nötiges Paket: nlme\n\n\nWarning: Paket 'nlme' wurde unter R Version 4.1.3 erstellt\n\n\n\nAttache Paket: 'nlme'\n\n\nDas folgende Objekt ist maskiert 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n\n\n\nAttache Paket: 'mgcv'\n\n\nDie folgenden Objekte sind maskiert von 'package:gam':\n\n    gam, gam.control, gam.fit, s\n\n# Fit a non-linear regression model using GAM\nmodel_gam &lt;- gam(label ~ s(followers_standardized) + s(friends_standardized), data = train_data)\n\n# Preprocess data by removing missing values\ncomplete_data &lt;- train_data[complete.cases(train_data), ]\n\n\n# Visualize the estimated effects of the variables\npar(mfrow = c(1, 2))  # Adjust the layout of the plots if desired\n\n# Plot the estimated effect of 'followers'\nplot(model_gam, select = 1, se=TRUE, residuals = TRUE,\n     xlab = \"Followers\", ylab = \"Effect of Followers on Label\")\n\n# Plot the estimated effect of 'friends'\nplot(model_gam, select = 2, se=TRUE, residuals = TRUE,\n     xlab = \"Friends\", ylab = \"Effect of Friends on Label\")\n\n\n\n\n\nlibrary('gratia')\n\nWarning: Paket 'gratia' wurde unter R Version 4.1.3 erstellt\n\n# plot all smooths\ndraw(model_gam)\n\n\n\n\nAdding partial residuals to the plot of the estimated effects in a GAM model provides a way to assess the goodness of fit and the presence of systematic patterns in the model.\nPartial residuals are calculated by subtracting the estimated component of the model (excluding the effect of a specific variable of interest) from the observed response variable. These residuals represent the part of the response variable that is not accounted for by the model for a specific variable.\nWhen partial residuals are added to the plot of the estimated effects, they are typically shown as individual points or a smooth line against the corresponding variable. This allows you to visually inspect the relationship between the variable and the unexplained variation in the response variable, which can help identify any remaining patterns or trends not captured by the model.\nBy examining the distribution and pattern of the partial residuals, you can assess whether the model captures the underlying structure and if there are any systematic deviations from the model’s predictions. Systematic patterns in the partial residuals may indicate that the model is misspecified or that there are additional variables or nonlinear relationships that should be considered.\n\ndraw(model_gam, residuals = TRUE)"
  }
]