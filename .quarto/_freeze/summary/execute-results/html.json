{
  "hash": "756905766dc15ad4cf62e3504f0a4c38",
  "result": {
    "markdown": "# Use case Twitter text mining\n\n### Required `R`packages {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npkgs <- c(\"kaggler\", \"tidyverse\", \"httr\", \"readr\", \"here\") \n\nlapply(pkgs, require, character.only = TRUE)\n#else install kaggler:\n#install.packages(c(\"devtools\"))\n#devtools::install_github(\"ldurazo/kaggler\")\n\nfilter <- dplyr::filter # to resolve namespace conflicts between tidyverse and dplyr\n\nlibrary(academictwitteR)    # collecting Twitter data\nlibrary(dplyr)              # Data Wrangling, pre-processing\nlibrary(quanteda)           # text-based descriptive\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(tidytext)           # Text Mining 'tidy format'\nlibrary(textclean)          # Pre-Processing + harmonizing\nlibrary(lubridate)          # Wrangling with time series data\nlibrary(ggplot2)\nlibrary(anytime)            # parsing dates\nlibrary(scales)\nlibrary(caret)\n\ntheme_set(theme_light())\n```\n:::\n\n\n#### Descriptive Overview\n\nFor demonstration purposes we retrieve an archived Twitter (sub-)sample, containing English **depression** specific tweets. (The sample contains *N* = 20,000 tweets.) We retrieve a sub-sample from the platform *Kaggle*.\n\n**a.)** This can be directly downloaded: [**Kaggle Data Dump, Depression\"**](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv)\n\n**b.)** Or by issuing a `GET` request via its `API` (see below).\n\nFor obtaining the data via the `Kaggle API` we *install* and *load* all dependencies to use `kaggler`.\n\n*Note*. For both (a & b authentication is required)\n\nA `kaggle.json` file needs to be made for **authentication** on Kaggle. The `kaggle.json` file should be positioned within the working directory. The json contains: *username* and the `API token`.\n\nTo create your `API token` go to your Kaggle Profil: (Your profile \\> Account \\> Settings \\> API \\> Create new token)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd(\"C:/Users/batzdova/Desktop\")\nlibrary(here)\nhere::i_am(\"kaggle.json\")\nkgl_auth(creds_file = 'kaggle.json')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse <- kgl_datasets_download_all(owner_dataset = \"infamouscoder/mental-health-social-media?select=Mental-Health-Twitter\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\ndataset <- httr::GET(\"https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv\")\n\ntemp <- tempfile()\ndownload.file(dataset$url,temp)\ndata <- read.csv(unz(temp, \"Mental-Health-Twitter.csv\"))\nunlink(temp)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmental <- data\n```\n:::\n\n\nOr alternatively, read in csv-data which have been downloaded.\n\n\n::: {.cell}\n\n:::\n\n\n### Overview of observations\n\nOur data set comprises $20,000$ observations with $10,000$ depression labeled tweets. Yet, only $19,881$ are unique tweets (i.e. `post_id`)\n\nEach observed person `user_id` ($N=72$ ) can be characterized by a set of variables (i.e., *features*) such as date of post creating `post_created` or count of `followers`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(mental)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 20,000\nColumns: 11\n$ ...1         <dbl> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,~\n$ post_id      <dbl> 6.378947e+17, 6.378904e+17, 6.377493e+17, 6.3~\n$ post_created <chr> \"Sun Aug 30 07:48:37 +0000 2015\", \"Sun Aug 30~\n$ post_text    <chr> \"It's just over 2 years since I was diagnosed~\n$ user_id      <dbl> 1013187241, 1013187241, 1013187241, 101318724~\n$ followers    <dbl> 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 8~\n$ friends      <dbl> 211, 211, 211, 211, 211, 211, 211, 211, 211, ~\n$ favourites   <dbl> 251, 251, 251, 251, 251, 251, 251, 251, 251, ~\n$ statuses     <dbl> 837, 837, 837, 837, 837, 837, 837, 837, 837, ~\n$ retweets     <dbl> 0, 1, 0, 2, 1, 1, 1, 0, 0, 41, 1, 0, 0, 0, 0,~\n$ label        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n```\n:::\n\n```{.r .cell-code}\nmental %>% \n  filter(label ==1) %>% \n  count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 1\n      n\n  <int>\n1 10000\n```\n:::\n\n```{.r .cell-code}\nn_distinct(mental$post_id) # count of unique tweets\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 19881\n```\n:::\n\n```{.r .cell-code}\nn_distinct(mental$user_id) # count of unique users\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 72\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmental %>%\n  mutate(no_mentions = str_remove_all(string = post_text, pattern = \"[@][\\\\w_-]+\")) %>% \n  dplyr::select(post_text, no_mentions) %>% \n  View()\n```\n:::\n\n\n\n\n**Parse Date Variable** `post_created`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove the timezone offset from the date string\nmental$date <- gsub(\" \\\\+\\\\d{4}\", \"\", mental$post_created)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the date string to a date variable\nmental$date_any <- anytime(mental$date)\n\n# Format the date variable as \"YYYY-MM-DD H:M\"\n#mental$formatted_date <- format(mental$date_any , \"%Y-%m-%d %H:%M\")\nmental$formatted_date <- format(mental$date_any , \"%Y-%m-%d\")\nmental$day_hour <- format(mental$date_any, \"%Y-%m-%d %H\")\n```\n:::\n\n\n`Formatted_date` is a `character` class. When we would use `as.Date()` we would lose the `timestamp` information. When we have a character with both date and time we have to use `POSIXct` class (as seen with the `date_any` variable).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmental$date <- as.Date(mental$formatted_date)\nmental$date_hour <- as.POSIXlt(mental$day_hour)\n#mental$formatted_date_time <- as.POSIXlt(mental$formatted_date)\n\n\nmental %>%\n\n# calculate how many tweets posted on each date\n  count(date) %>%\n\n# plot the result\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  theme_bw() +\n  scale_x_date(date_breaks = \"7 day\", date_labels = \"%b %d\") +\n  ylab(\"Number of Tweets\\n\") + xlab(\"\\nDate\")\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-10-1.png){width=100%}\n:::\n:::\n\n\n**Distribution Friends**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmental$logfriends <-log(mental$friends+1)\n\nhist(mental$logfriends,  breaks = 50, prob= TRUE)\nlines(density(mental$logfriends), col = \"red\")  \n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-11-1.png){width=100%}\n:::\n:::\n\n\n**Distribution Followers**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmental$logfollow=log(mental$followers+1)\nhist(mental$logfollow, breaks = 50, prob = TRUE)\nlines(density(mental$logfollow), col = \"red\")   \n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/Log transformed followers-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mental, aes(x = logfriends, y = logfollow)) + \n  geom_point(alpha=.5)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n\n**Tweets und Retweets per day**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwt_retwt_summary <- mental %>%\n  group_by(user_id, date, label) %>%\n  summarize(tweets = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1)\n\nstartTime <- as.Date(\"2014-01-01\")\nendTime <- as.Date(\"2017-10-31\")\n\n# create a start and end time R object\nstart.end <- c(startTime,endTime)\nstart.end\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2014-01-01\" \"2017-10-31\"\n```\n:::\n\n```{.r .cell-code}\npl1 <- twt_retwt_summary %>%\n  ggplot(aes(date, tweets, group = user_id, fill= user_id)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\n\ntwt_retwt_summary %>%\n  ggplot(aes(date, tweets, group = user_id, fill= label, col= label)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-13-1.png){width=100%}\n:::\n\n```{.r .cell-code}\ntwt_retwt_summary %>%\n  ggplot(aes(date, avg_retweets, group = user_id, fill= label, col= label)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-13-2.png){width=100%}\n:::\n\n```{.r .cell-code}\npl2 <-twt_retwt_summary %>%\n  ggplot(aes(date, avg_retweets, group=user_id)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Average (geometric mean) retweets each day\")+\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\ngridExtra::grid.arrange(pl1,pl2, ncol=2)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-13-3.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nts.plot(twt_retwt_summary$tweets)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-14-1.png){width=100%}\n:::\n\n```{.r .cell-code}\nts.plot(twt_retwt_summary$avg_retweets)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-14-2.png){width=100%}\n:::\n:::\n\n\n**User Activity**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmental %>%\n  count(user_id, sort = TRUE) %>%\n  head(12) %>%\n  mutate(username = reorder(user_id, n)) %>%\n  ggplot(aes(username, n)) +\n  geom_col() +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-15-1.png){width=100%}\n:::\n\n```{.r .cell-code}\n#Retweet activity per user\nmental %>%\n  arrange(desc(retweets)) %>%\n  select(user_id, post_text, retweets)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20,000 x 3\n      user_id post_text                                     retweets\n        <dbl> <chr>                                            <dbl>\n 1  490044008 RT @omgAdamSaleh: We got kicked out of a @De~   839540\n 2 3249600438 RT @POTUS: Thank you for everything. My last~   822450\n 3 1497350173 RT @POTUS: Thank you for everything. My last~   820245\n 4 1306425758 RT @Louis_Tomlinson: Our fans support really~   402946\n 5 1497350173 RT @HamillHimself: no words #Devastated http~   358548\n 6 3249600438 RT @HamillHimself: no words #Devastated http~   358536\n 7  171999132 RT @HamillHimself: no words #Devastated http~   358510\n 8 1306425758 RT @Louis_Tomlinson: @NaughtyBoyMusic Jesus ~   341695\n 9 1306425758 RT @Louis_Tomlinson: @NaughtyBoyMusic good f~   306473\n10  490044008 RT @edsheeran: Hello 2017... https://t.co/1U~   273872\n# i 19,990 more rows\n```\n:::\n\n```{.r .cell-code}\n#Tweets und Retweets per User\nmental %>%\n  group_by(user_id) %>%\n  summarize(tweets = n(),\n            retweetnr = sum(retweets)) %>%\n  arrange(desc(tweets)) %>%\n  arrange(desc(retweetnr))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 72 x 3\n      user_id tweets retweetnr\n        <dbl>  <int>     <dbl>\n 1 1497350173    792   7004039\n 2  490044008   2117   5379581\n 3 1306425758    306   4810463\n 4 3045320288    633   4402064\n 5 3249600438   1504   4131286\n 6  324294391    215    765268\n 7  171999132    121    463537\n 8   18831261    543    317468\n 9   49548465    352    197019\n10 1015648590     84    175150\n# i 62 more rows\n```\n:::\n\n```{.r .cell-code}\n#Proportion Likes and Retweets per User\nmental %>%\n  select(user_id, post_text, retweets, favourites) %>%\n  mutate(ratio = (favourites + 1) / (retweets + 1)) %>% # to avoid zero\n  arrange(desc(ratio))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20,000 x 5\n      user_id post_text                    retweets favourites ratio\n        <dbl> <chr>                           <dbl>      <dbl> <dbl>\n 1 2660477449 \"@BenBuckwalter i was diagn~        0      39008 39009\n 2 2660477449 \"@JordanSweeto i hope whate~        0      39008 39009\n 3 2660477449 \"@BenBuckwalter i would!\"           0      39008 39009\n 4 2660477449 \"@BenBuckwalter i feel like~        0      39008 39009\n 5 2660477449 \"@BryanStars I love you,  y~        0      39008 39009\n 6 2660477449 \"@BenBuckwalter me! I volun~        0      39008 39009\n 7 2660477449 \"I'm getting tired of your ~        0      39008 39009\n 8 2660477449 \"@deefizzy you're amazing. ~        0      39008 39009\n 9 2660477449 \"@BenBuckwalter people tend~        0      39008 39009\n10 2660477449 \"@BenBuckwalter being notic~        0      39008 39009\n# i 19,990 more rows\n```\n:::\n:::\n\n\n#### 8. Basics Text Mining in a Nutshell\n\nA.**Document** = collection of strings and their metadata\n\nB.**Corpus** = collection of \\*documents\\*\\*.\n\nC.**Tokens** = smallest unit of meaning (mostly words)\n\nD.**Vocabulary** = collection of unique words of a *Corpus*.\n\nE.**D**ocument-**F**eature-**M**atrix or **D**ocument-**T**erm-**M**atrix =\n\nthis is a matrix where the rows indicate the documents, the columns indicate the expressions and the cells indicate the frequency of the expressions in each document, i.e.: Matrix with *n* = number of Document rows and *m* = size of Vocabulary columns, based on **bag-of-words** assumption, which ignores word order and syntax.\n\n##### 8.1. Pre-processing\n\nA. **Tokenisation**\n\nB. **Remove** Stopwords\n\nC. **String** operations (punctuation, normalise URL).\n\nThe order of the application steps is crucial and should be guided by the research question.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Clean @-Symbol\nmental$post_text<- gsub(\"@\\\\w+\", \"\", mental$post_text)\n\n# Clean Interpunction\nmental$post_text<- gsub(\"[[:punct:]]\", \"\", mental$post_text)\n\n# Clean Numbers\nmental$post_text <- gsub(\"[[:digit:]]\", \"\", mental$post_text)\n\n# Clean pictures\nmental$post_text<- gsub(\"pictwitter\\\\w+ *\", \"\", mental$post_text)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Clean HTML-Notation\nremove_html <- \"&amp;|&lt;|&gt;\" #&lt und &gt stehen für < und > und &amp für &\n\nmental_en <- mental %>% \n  select(post_id, post_text, user_id) %>% \n  mutate(text = stringr::str_remove_all(post_text, remove_html))\n```\n:::\n\n\nWith [`textclean`](https://github.com/trinker/textclean) further harmonisation can be done, e.g. replacing emojis with word equivalents.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmental_en$text <- replace_emoji(mental_en$text)\n```\n:::\n\n\nOr normalizing colloquial expressions such as word elongations (e.g., \"Whyyyy\")\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmental_en$text <- replace_word_elongation(mental_en$text)\n```\n:::\n\n\nFrom the data frame, you can create a *Corpus* object, i.e. a collection of *documents* (tweets) and their *metadata*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#select unique tweet_ids\nunique <- mental_en %>% group_by(post_id) %>% unique()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets_en_corpus <- corpus(unique,\n                           docid_field = \"post_id\",\n                           text_field = \"post_text\")\n```\n:::\n\n\nIn `quanteda` the smallest meaningful unit are **tokens** which can be generated by segmenting into words (see `tidytext` `unnest_tokens(word, text)`. Further, we can remove **stopwords** (hedges, grammatical signifiers which bear little semantic meaning). See (`stopwords()`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets_en_tokens <- tokens(tweets_en_corpus,\n                           remove_punct = TRUE,\n                           remove_numbers = TRUE,\n                           remove_symbols = TRUE,\n                           remove_url = TRUE) %>% \n  tokens_tolower() %>% \n  tokens_remove(stopwords(\"english\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nkw_depression <- kwic(tweets_en_tokens, pattern =  \"depress*\", window = 3)\nhead(kw_depression)\n```\n:::\n\n\n#### 8.2. Tidy Workflow--Exploration Tweets, Retweets and Hashtags\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Tokenising Tweets (1-gram) and Pre-processing with tidytext\n\ntweet_words <- mental %>%\n  select(user_id, \n         post_text, \n         retweets, \n         favourites,\n         formatted_date, \n         post_id) %>%\n  unnest_tokens(word, post_text) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  filter(!word %in% c(\"amp\", \"de\", \"twitter\", \"youre\", \"en\", \"die\", \"rt\", \"im\", \"dont\", \"ive\", \"yong\"),\n         stringr::str_detect(word, \"[a-z]\"))\n\ntweet_words %>%\n  count(word, sort = TRUE) %>%\n  head(16) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot2::ggplot(aes(word, n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Most common words in Tweets on Depression\",\n       y = \"Frequency of words\")\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/Tokenisation und Pre-processing-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Distribution of Favorites\nmental %>%\n  ggplot2::ggplot(aes(favourites + 1)) +\n  geom_histogram() +\n  scale_x_log10()\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/Favorites und Retweets-1.png){width=100%}\n:::\n\n```{.r .cell-code}\n#Mean Retweets and Mean Favorites\nword_summary <- tweet_words  %>%\n  group_by(word) %>%\n  summarize(n = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1,\n            avg_favorites = exp(mean(log(favourites + 1))) - 1) %>%\n  filter(n >= 30) %>%\n  arrange(desc(avg_retweets))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Tf-idf: signals importance of a word in a corpus selection\n\ntop_word <- tweet_words %>%\n  count(word, formatted_date) %>%\n  bind_tf_idf(word, formatted_date, n) %>%\n  arrange(desc(tf_idf)) %>%\n  distinct(formatted_date, .keep_all = TRUE) %>%\n  arrange(formatted_date)\n\ntf_idf <- word_summary %>%\n  inner_join(top_word, by = c(\"word\")) %>%\n  arrange(desc(avg_retweets)) \n\n#Tf-idf of words per hour\ntf_idf %>% \n  ggplot2::ggplot(aes(tf_idf, forcats::fct_reorder(word, tf_idf), fill = word)) +\n  geom_col(show.legend = FALSE)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/tf-idf-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Hashtag exploration\ntweet_words <- mental %>%\n  mutate(hashtags = stringr::str_count(post_text, \"#[a-zA-Z]\"), sort = TRUE) %>%\n  select(user_id, post_text, retweets, favourites, formatted_date , post_id,\n        hashtags) %>%\n  unnest_tokens(word, post_text) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  filter(!word %in% c(\"de\", \"|\"),\n         stringr::str_detect(word, \"[a-z]\"))\n\n\nmental %>%\n  mutate(hashtags = stringr::str_count(post_text, \"#[a-zA-Z]\"), sort = TRUE) %>%\n  filter(hashtags < 6) %>%\n  group_by(user_id) %>%\n  summarize(tweets = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1) %>%\n  filter(tweets >= 30) %>%\n  arrange(desc(avg_retweets))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 55 x 3\n   user_id tweets avg_retweets\n     <dbl>  <int>        <dbl>\n 1 3.05e 9    633       229.  \n 2 1.50e 9    792        74.8 \n 3 7.40e 7     50        72.4 \n 4 1.31e 9    306        59.8 \n 5 1.72e 8    121        11.2 \n 6 3.09e 7    142        11.1 \n 7 3.25e 9   1504         6.85\n 8 7.63e17    522         6.79\n 9 6.32e 8    144         5.09\n10 4.95e 7    352         4.26\n# i 45 more rows\n```\n:::\n\n```{.r .cell-code}\ntweet_word_summary <- tweet_words %>%\n  filter(hashtags < 6) %>%\n  group_by(word) %>%\n  summarize(n = n(),\n            avg_retweets = exp(mean(log(retweets+ 1))) - 1,\n            avg_favorites = exp(mean(log(favourites + 1))) - 1)\n\n\n\ntweet_word_summary <-tweet_word_summary %>%\n  filter(n >= 100,\n         !stringr::str_detect(word, \"https\")) %>%\n   filter(!word %in% c(\"amp\", \"de\", \"twitter\", \"youre\", \"en\", \"die\", \"rt\", \"im\", \"dont\", \"ive\", \"yong\"),\n         stringr::str_detect(word, \"[a-z]\")) %>% \n  arrange(desc(avg_retweets)) %>% \n  head()\n\ntweet_word_summary %>%\n  filter(n >= 100,\n         !stringr::str_detect(word, \"https\")) %>%\n  ggplot2::ggplot(aes(n, avg_retweets)) +\n  geom_point() +\n  geom_text(aes(label = word), check_overlap = TRUE) +\n  scale_x_log10() +\n  scale_y_log10()\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/hashtags-1.png){width=100%}\n:::\n\n```{.r .cell-code}\ntweet_word_summary %>%\n  filter(n >= 100,\n         !stringr::str_detect(word, \"https\")) %>%\n  arrange(desc(avg_retweets)) %>%\n  head(20) %>%\n  mutate(word = reorder(word, avg_retweets)) %>%\n  ggplot2::ggplot(aes(word, avg_retweets)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Which words get the most retweets in depression?\",\n       subtitle = \"Only words appearing in at least 100 tweets\",\n       y = \"Geometric mean of the Number of retweets\")\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/hashtags-2.png){width=100%}\n:::\n:::\n\n\n\n\n# Predicting depression tweets based on followers and friends\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data preprocessing\n# Check for missing values\nmissing_values <- sum(is.na(mental))\nif (missing_values > 0) {\n  data <- na.omit(mental)  # Remove rows with missing values\n}\n\n# Convert label variable to factor\n#mental$label <- as.factor(mental$label)\n\n# Standardize the followers and friends variables\nmental$followers_standardized <- as.numeric(scale(mental$followers))\nmental$friends_standardized <- as.numeric(scale(mental$friends))\n\n# Check the standardized variables\nhead(mental[, c(\"followers\", \"followers_standardized\", \"friends\", \"friends_standardized\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 4\n  followers followers_standardized friends friends_standardized\n      <dbl>                  <dbl>   <dbl>                <dbl>\n1        84                 -0.430     211               -0.311\n2        84                 -0.430     211               -0.311\n3        84                 -0.430     211               -0.311\n4        84                 -0.430     211               -0.311\n5        84                 -0.430     211               -0.311\n6        84                 -0.430     211               -0.311\n```\n:::\n\n```{.r .cell-code}\n# Select variables for analysis\nselected_vars <- c(\"label\", \"followers_standardized\", \"friends_standardized\")\nmental_selected <- mental[selected_vars]\n\n\n# Split the dataset into training and testing sets\nset.seed(123)  # Set a seed for reproducibility\ntrain_indices <- createDataPartition(mental_selected$label, p = 0.8, list = FALSE)\ntrain_data <- mental_selected[train_indices, ]\ntest_data <- mental_selected[-train_indices, ]\n\n\ntrain_data$label <- as.numeric(train_data$label)\ntest_data$label <- as.numeric(test_data$label)\n\n# Linear Regression model\nmodel <- lm(label ~ followers_standardized + friends_standardized, data = train_data)\n\n# Model evaluation\npredictions <- predict(model, newdata = test_data)\nevaluation <- data.frame(actual = test_data$label, predicted = predictions)\n\n# Calculate evaluation metrics\nrmse <- mean((evaluation$actual - evaluation$predicted)^2)\nrmse <- sqrt(rmse)\nr_squared <- cor(evaluation$actual, evaluation$predicted)^2\n\n# Print evaluation metrics\ncat(\"Mean Squared Error (MSE):\", rmse, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error (MSE): 0.4776674 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot Mean Squared Error (RMSE): 0.4776674 \n```\n:::\n\n```{.r .cell-code}\ncat(\"R-squared value:\", r_squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared value: 0.08754619 \n```\n:::\n\n```{.r .cell-code}\n# Regression coefficients and p-values\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = label ~ followers_standardized + friends_standardized, \n    data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8265 -0.4814 -0.0809  0.4749  1.1280 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             0.499941   0.003789  131.94   <2e-16 ***\nfollowers_standardized -0.286159   0.008359  -34.23   <2e-16 ***\nfriends_standardized    0.314163   0.008355   37.60   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4793 on 15997 degrees of freedom\nMultiple R-squared:  0.08129,\tAdjusted R-squared:  0.08117 \nF-statistic: 707.7 on 2 and 15997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nlibrary(dotwhisker)\ndwplot(model)+  \n   xlab(\"Coefficient Estimate\") + ylab(\"\") +\n    geom_vline(xintercept = 0,\n               colour = \"grey60\",\n               linetype = 2) +\n  ggtitle(\"Predicting Depression Tweets\") \n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-19-1.png){width=100%}\n:::\n:::\n\n\nThe followers_standardized coefficient (-0.286159) indicates that, on average, a one-standard-deviation increase in the standardized followers variable is associated with a decrease of 0.286159 units in the predicted depression label.\nThe friends_standardized coefficient (0.314163) indicates that, on average, a one-standard-deviation increase in the standardized friends variable is associated with an increase of 0.314163 units in the predicted depression label. All t-values are large (absolute values > 2), indicating that the coefficients are significantly different from zero.\n\n\nOverall, the regression analysis suggests that both the standardized followers and standardized friends variables have a statistically significant association with the predicted depression label. An increase in the standardized followers is associated with a decrease in the predicted depression label, while an increase in the standardized friends is associated with an increase in the predicted depression label.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a non-linear regression model\nmodel_nonlinear <- nls(label ~ a * exp(b * followers_standardized) + c * friends_standardized,\n                       data = train_data,\n                       start = list(a = 1, b = 1, c = 1))\n\n# Print the model summary\nsummary(model_nonlinear)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFormula: label ~ a * exp(b * followers_standardized) + c * friends_standardized\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na  0.478974   0.004230  113.23   <2e-16 ***\nb -0.510885   0.024174  -21.13   <2e-16 ***\nc  0.095444   0.003802   25.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4864 on 15997 degrees of freedom\n\nNumber of iterations to convergence: 17 \nAchieved convergence tolerance: 3.114e-07\n```\n:::\n\n```{.r .cell-code}\n# Predict the label using the non-linear regression model\ntest_data$predicted_nonlinear <- predict(model_nonlinear, newdata = test_data)\n\n# Evaluate the non-linear regression model\nmse_nonlinear <- mean((test_data$label - test_data$predicted_nonlinear)^2)\nrmse_nonlinear <- sqrt(mse_nonlinear)\nr_squared_nonlinear <- 1 - sum((test_data$label - test_data$predicted_nonlinear)^2) / sum((test_data$label - mean(test_data$label))^2)\n\n# Print evaluation metrics\ncat(\"Mean Squared Error (MSE) - Nonlinear Regression:\", mse_nonlinear, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error (MSE) - Nonlinear Regression: 0.2366316 \n```\n:::\n\n```{.r .cell-code}\ncat(\"Root Mean Squared Error (RMSE) - Nonlinear Regression:\", rmse_nonlinear, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot Mean Squared Error (RMSE) - Nonlinear Regression: 0.4864479 \n```\n:::\n\n```{.r .cell-code}\ncat(\"R-squared value - Nonlinear Regression:\", r_squared_nonlinear, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR-squared value - Nonlinear Regression: 0.05347357 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndwplot(model_nonlinear)+  \n   xlab(\"Coefficient Estimate\") + ylab(\"\") +\n    geom_vline(xintercept = 0,\n               colour = \"grey60\",\n               linetype = 2) +\n  ggtitle(\"Predicting Depression Tweets with a non-linear model\") \n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-21-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gam)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'gam' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLade nötiges Paket: splines\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLade nötiges Paket: foreach\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttache Paket: 'foreach'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDie folgenden Objekte sind maskiert von 'package:purrr':\n\n    accumulate, when\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded gam 1.22\n```\n:::\n\n```{.r .cell-code}\nlibrary(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'mgcv' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLade nötiges Paket: nlme\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'nlme' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttache Paket: 'nlme'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDas folgende Objekt ist maskiert 'package:dplyr':\n\n    collapse\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttache Paket: 'mgcv'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nDie folgenden Objekte sind maskiert von 'package:gam':\n\n    gam, gam.control, gam.fit, s\n```\n:::\n\n```{.r .cell-code}\n# Fit a non-linear regression model using GAM\nmodel_gam <- gam(label ~ s(followers_standardized) + s(friends_standardized), data = train_data)\n\n# Preprocess data by removing missing values\ncomplete_data <- train_data[complete.cases(train_data), ]\n\n\n# Visualize the estimated effects of the variables\npar(mfrow = c(1, 2))  # Adjust the layout of the plots if desired\n\n# Plot the estimated effect of 'followers'\nplot(model_gam, select = 1, se=TRUE, residuals = TRUE,\n     xlab = \"Followers\", ylab = \"Effect of Followers on Label\")\n\n# Plot the estimated effect of 'friends'\nplot(model_gam, select = 2, se=TRUE, residuals = TRUE,\n     xlab = \"Friends\", ylab = \"Effect of Friends on Label\")\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-22-1.png){width=100%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('gratia')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Paket 'gratia' wurde unter R Version 4.1.3 erstellt\n```\n:::\n\n```{.r .cell-code}\n# plot all smooths\ndraw(model_gam)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-23-1.png){width=100%}\n:::\n:::\n\n\nAdding partial residuals to the plot of the estimated effects in a GAM model provides a way to assess the goodness of fit and the presence of systematic patterns in the model.\n\nPartial residuals are calculated by subtracting the estimated component of the model (excluding the effect of a specific variable of interest) from the observed response variable. These residuals represent the part of the response variable that is not accounted for by the model for a specific variable.\n\nWhen partial residuals are added to the plot of the estimated effects, they are typically shown as individual points or a smooth line against the corresponding variable. This allows you to visually inspect the relationship between the variable and the unexplained variation in the response variable, which can help identify any remaining patterns or trends not captured by the model.\n\nBy examining the distribution and pattern of the partial residuals, you can assess whether the model captures the underlying structure and if there are any systematic deviations from the model's predictions. Systematic patterns in the partial residuals may indicate that the model is misspecified or that there are additional variables or nonlinear relationships that should be considered.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw(model_gam, residuals = TRUE)\n```\n\n::: {.cell-output-display}\n![](summary_files/figure-html/unnamed-chunk-24-1.png){width=100%}\n:::\n:::\n",
    "supporting": [
      "summary_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}