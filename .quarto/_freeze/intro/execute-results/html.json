{
  "hash": "a43c8124076096e588792511bc961e45",
  "result": {
    "markdown": "# Introduction\n\n\n\n\n\n#### 1. Basic workflow for working with text data\n\n##### A) **Sampling Text Data**\n\n-   Existing Archives (e.g., [Reddit Pushift Data](https://files.pushshift.io/reddit/comments/), see also [Baumgartner et al., 2020](https://cs.paperswithcode.com/paper/the-pushshift-reddit-dataset))\n-   APIs (e.g., [The Guardian](https://cran.r-project.org/web/packages/guardianapi/guardianapi.pdf))\n-   Web Scraping (e.g., with [rvest](https://rvest.tidyverse.org/)) (or just for fun, build you own API with [plumber](https://www.rplumber.io/))\n\n##### B) **Pre-processing/ Data Wrangling**\n\n-   Cleaning\n-   Selecting and Weightning of *Features* (Reducing dimensionality)\n\n##### C) **Analyses**\n\n-   *Dictionary*- or rulebased Approaches\n-   Supervised Approaches (e.g., SVM)\n-   Unsupervised Approaches (e.g., Topic Modeling)\n\n##### D) **Validation**\n\n-   e.g., Reliability of Bot Classifications ([tweetbotornot2](https://github.com/mkearney/tweetbotornot2#botometer))\n\n#### 2. Data Sampling\n\n##### A) **Screen Scraping**:\n\n-   *Scraping*, *Parsing* and *Formatting* (e.g., with *Rselenium*)\n\n##### B) **API Access Points**:\n\n-   Send **Get**-Requests directly to the data base\n-   Gateways for specific data types, irrespective of the programming language\n\n##### B.1. Application Programming Interfaces (APIs)\n\n-   \"communicates\" directly with the data base\n-   determines *which* information are accessible for *whom*, *how* and to which *degree*\n\n##### B.2. API-Applications\n\n-   Embedded content into other applications\n-   Build bots (e.g., [Telegram](https://core.telegram.org/bots/api))\n-   Collect data for market research\n\nAccess point exist for:\n\n-   [**Youtube**](https://developers.google.com/youtube/v3) -- allows via keywords to search for contents, the Video, lists and user activities such as upvoting, comments, favorites\n\n-   [**Instagram**](https://github.com/digitalmethodsinitiative/dmi-instascraper/) -- allows to search for comment structures relating to postings, friends information of users or geolocation\n\n-   [**Wikipedia**](https://cran.r-project.org/web/packages/WikipediR/WikipediR.pdf) -- allows to search for MediaWiki revisions, revision summaries connected to an entry, timestamps, site information, user information\n\n-   [**Google Maps**](https://cran.rstudio.com/web/packages/mapsapi/vignettes/intro.html) -- allows to search for coordinates of latitude and longitude, distance matrices\n\n##### B.3. Advantages of API harvesting\n\n-   No interaction with HTML data types necessary (Output: `JSON`-files)\n-   Usually legal (upon following the Terms of Service)\n\n##### B.4. Disadvantages of API harvesting\n\n-   Not every website has an API\n-   Only the data that the API makes available can be retrieved\n-   *Rate limitations* (e.g. number of tweets per day/ query).\n-   Terms of use and changes to the API restrict use (e.g. code reproducibility, sharing of data sets)\n-   Code varies depending on platform and level of documentation detail\n\n#### 3. Tool overview for Twitter data access\n\nTwitter for research on the dynamics of fast-moving socio-political events and contemporary culture.\n\nDifferentiation by requirements:\n\n-   Graphical User Interface\n-   Type of data\n-   Suitability for collection or for processing and analysis\n-   API version (e.g. `Twitter API v1.1` offer the packages `rtweet`)\n\n**Sources for Tools**:\n\n-   [Twitter Tool List: Wiki of the Social Media Observatory am Leibniz-Institut für Medienforschung \\| Hans-Bredow-Institut (HBI)](https://smo-wiki.leibniz-hbi.de/Twitter-Tools)\n\n-   Tutorial for Twitter-Daten: Jürgens, P., & Jungherr, A. (2016). A tutorial for using Twitter data in the social sciences: Data collection, preparation, and analysis. Preparation, and Analysis (January 5, 2016).)\n\n-   [Tools for Collecting and Analysing Social Media Data](https://wiki.digitalmethods.net/Dmi/ToolDatabase)\n\n#### 4. Academic Twitter Access Point (v2 API endpoints)\n- To run the following code examples, [Academic Research Access](https://developer.twitter.com/en/products/twitter-api/academic-research) for the Twitter API v2 is required. To sample Twitter data, one uses the `R` package [`academictwitteR`](https://cran.r-project.org/web/packages/academictwitteR/academictwitteR.pdf).\n\n- In the framework, one needs the so-called [Bearer Token](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens). This [vignette](https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-auth.html) in the `academictwitteR` package explains the process of gaining access to the Twitter API.\n\n\n#### 4.1. Twitter Modi with v2 API Endpoints\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Tweet-centric sampling (samples 1000 tweets)\n\nget_all_tweets(\n  query = 'xyz',\n  start_tweets,\n  end_tweets,\n  bearer_token = get_bearer(),\n  n = 1000\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# User-centric sampling: defining users\n\nusers <- c(\"juliasilge\", \"drob\")\n\nget_user_id(users, bearer_token)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# User-centric sampling (samples 100 tweets)\n\nget_user_timeline(\nuserid, #single string or Vector with User_Ids\nstart_tweets,\nend_tweets,\nbearer_token = get_bearer(),\nn = 100,\n)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}