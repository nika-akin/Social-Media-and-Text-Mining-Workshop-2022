{"title":"Use case Twitter text mining","markdown":{"headingText":"Use case Twitter text mining","containsRefs":false,"markdown":"\n### Required `R`packages {.unnumbered}\n\n```{r, message = FALSE, warning = FALSE, results = 'hide'}\npkgs <- c(\"kaggler\", \"tidyverse\", \"httr\", \"readr\", \"here\") \n\nlapply(pkgs, require, character.only = TRUE)\n#else install kaggler:\n#install.packages(c(\"devtools\"))\n#devtools::install_github(\"ldurazo/kaggler\")\n\nfilter <- dplyr::filter # to resolve namespace conflicts between tidyverse and dplyr\n\nlibrary(academictwitteR)    # collecting Twitter data\nlibrary(dplyr)              # Data Wrangling, pre-processing\nlibrary(quanteda)           # text-based descriptive\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(tidytext)           # Text Mining 'tidy format'\nlibrary(textclean)          # Pre-Processing + harmonizing\nlibrary(lubridate)          # Wrangling with time series data\nlibrary(ggplot2)\nlibrary(anytime)            # parsing dates\nlibrary(scales)\nlibrary(caret)\n\ntheme_set(theme_light())\n```\n\n#### Descriptive Overview\n\nFor demonstration purposes we retrieve an archived Twitter (sub-)sample, containing English **depression** specific tweets. (The sample contains *N* = 20,000 tweets.) We retrieve a sub-sample from the platform *Kaggle*.\n\n**a.)** This can be directly downloaded: [**Kaggle Data Dump, Depression\"**](https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv)\n\n**b.)** Or by issuing a `GET` request via its `API` (see below).\n\nFor obtaining the data via the `Kaggle API` we *install* and *load* all dependencies to use `kaggler`.\n\n*Note*. For both (a & b authentication is required)\n\nA `kaggle.json` file needs to be made for **authentication** on Kaggle. The `kaggle.json` file should be positioned within the working directory. The json contains: *username* and the `API token`.\n\nTo create your `API token` go to your Kaggle Profil: (Your profile \\> Account \\> Settings \\> API \\> Create new token)\n\n```{r demonstration, eval=FALSE, error=FALSE}\nsetwd(\"C:/Users/batzdova/Desktop\")\nlibrary(here)\nhere::i_am(\"kaggle.json\")\nkgl_auth(creds_file = 'kaggle.json')\n```\n\n```{r eval=FALSE}\nresponse <- kgl_datasets_download_all(owner_dataset = \"infamouscoder/mental-health-social-media?select=Mental-Health-Twitter\")\n```\n\n```{r eval=FALSE}\nlibrary(httr)\ndataset <- httr::GET(\"https://www.kaggle.com/datasets/infamouscoder/mental-health-social-media?select=Mental-Health-Twitter.csv\")\n\ntemp <- tempfile()\ndownload.file(dataset$url,temp)\ndata <- read.csv(unz(temp, \"Mental-Health-Twitter.csv\"))\nunlink(temp)\n```\n\n```{r eval=FALSE}\nmental <- data\n```\n\nOr alternatively, read in csv-data which have been downloaded.\n\n```{r warning=FALSE, message=FALSE, echo=FALSE}\n#|warning=FALSE\n#|echo=FALSE\nsetwd(\"C:/Users/batzdova/Desktop\")\nhere::i_am(\"Mental-Health-Twitter.csv\")\nmental <- read_csv(\"Mental-Health-Twitter.csv\")\n```\n\n### Overview of observations\n\nOur data set comprises $20,000$ observations with $10,000$ depression labeled tweets. Yet, only $19,881$ are unique tweets (i.e. `post_id`)\n\nEach observed person `user_id` ($N=72$ ) can be characterized by a set of variables (i.e., *features*) such as date of post creating `post_created` or count of `followers`.\n\n```{r}\nglimpse(mental)\n\nmental %>% \n  filter(label ==1) %>% \n  count()\n\nn_distinct(mental$post_id) # count of unique tweets\nn_distinct(mental$user_id) # count of unique users\n```\n\n\n```{r}\nmental %>%\n  mutate(no_mentions = str_remove_all(string = post_text, pattern = \"[@][\\\\w_-]+\")) %>% \n  dplyr::select(post_text, no_mentions) %>% \n  View()\n```\n\n\n\n**Parse Date Variable** `post_created`\n\n```{r}\n# Remove the timezone offset from the date string\nmental$date <- gsub(\" \\\\+\\\\d{4}\", \"\", mental$post_created)\n```\n\n```{r}\n# Convert the date string to a date variable\nmental$date_any <- anytime(mental$date)\n\n# Format the date variable as \"YYYY-MM-DD H:M\"\n#mental$formatted_date <- format(mental$date_any , \"%Y-%m-%d %H:%M\")\nmental$formatted_date <- format(mental$date_any , \"%Y-%m-%d\")\nmental$day_hour <- format(mental$date_any, \"%Y-%m-%d %H\")\n```\n\n`Formatted_date` is a `character` class. When we would use `as.Date()` we would lose the `timestamp` information. When we have a character with both date and time we have to use `POSIXct` class (as seen with the `date_any` variable).\n\n```{r}\nmental$date <- as.Date(mental$formatted_date)\nmental$date_hour <- as.POSIXlt(mental$day_hour)\n#mental$formatted_date_time <- as.POSIXlt(mental$formatted_date)\n\n\nmental %>%\n\n# calculate how many tweets posted on each date\n  count(date) %>%\n\n# plot the result\n  ggplot(aes(x = date, y = n)) +\n  geom_line() +\n  theme_bw() +\n  scale_x_date(date_breaks = \"7 day\", date_labels = \"%b %d\") +\n  ylab(\"Number of Tweets\\n\") + xlab(\"\\nDate\")\n\n```\n\n**Distribution Friends**\n\n```{r }\nmental$logfriends <-log(mental$friends+1)\n\nhist(mental$logfriends,  breaks = 50, prob= TRUE)\nlines(density(mental$logfriends), col = \"red\")  \n```\n\n**Distribution Followers**\n\n```{r Log transformed followers}\nmental$logfollow=log(mental$followers+1)\nhist(mental$logfollow, breaks = 50, prob = TRUE)\nlines(density(mental$logfollow), col = \"red\")   \n```\n\n```{r}\nggplot(mental, aes(x = logfriends, y = logfollow)) + \n  geom_point(alpha=.5)\n```\n\n**Tweets und Retweets per day**\n\n```{r message=FALSE, warning=FALSE}\ntwt_retwt_summary <- mental %>%\n  group_by(user_id, date, label) %>%\n  summarize(tweets = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1)\n\nstartTime <- as.Date(\"2014-01-01\")\nendTime <- as.Date(\"2017-10-31\")\n\n# create a start and end time R object\nstart.end <- c(startTime,endTime)\nstart.end\n\npl1 <- twt_retwt_summary %>%\n  ggplot(aes(date, tweets, group = user_id, fill= user_id)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\n\ntwt_retwt_summary %>%\n  ggplot(aes(date, tweets, group = user_id, fill= label, col= label)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\ntwt_retwt_summary %>%\n  ggplot(aes(date, avg_retweets, group = user_id, fill= label, col= label)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Number of depression tweets per day\") +\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\npl2 <-twt_retwt_summary %>%\n  ggplot(aes(date, avg_retweets, group=user_id)) +\n  geom_line( size = 0.4) +\n  expand_limits(y = 0) +\n  labs(x = \"Time\",\n       y = \"Average (geometric mean) retweets each day\")+\n  (scale_x_date(limits=start.end,\n                             breaks=date_breaks(\"3 month\"),\n                             labels=date_format(\"%b %y\")))\n\ngridExtra::grid.arrange(pl1,pl2, ncol=2)\n```\n\n```{r warning=FALSE, message=FALSE}\nts.plot(twt_retwt_summary$tweets)\nts.plot(twt_retwt_summary$avg_retweets)\n```\n\n**User Activity**\n\n```{r warning=FALSE, message=FALSE}\nmental %>%\n  count(user_id, sort = TRUE) %>%\n  head(12) %>%\n  mutate(username = reorder(user_id, n)) %>%\n  ggplot(aes(username, n)) +\n  geom_col() +\n  coord_flip()\n\n#Retweet activity per user\nmental %>%\n  arrange(desc(retweets)) %>%\n  select(user_id, post_text, retweets)\n#Tweets und Retweets per User\nmental %>%\n  group_by(user_id) %>%\n  summarize(tweets = n(),\n            retweetnr = sum(retweets)) %>%\n  arrange(desc(tweets)) %>%\n  arrange(desc(retweetnr))\n\n#Proportion Likes and Retweets per User\nmental %>%\n  select(user_id, post_text, retweets, favourites) %>%\n  mutate(ratio = (favourites + 1) / (retweets + 1)) %>% # to avoid zero\n  arrange(desc(ratio))\n\n```\n\n#### 8. Basics Text Mining in a Nutshell\n\nA.**Document** = collection of strings and their metadata\n\nB.**Corpus** = collection of \\*documents\\*\\*.\n\nC.**Tokens** = smallest unit of meaning (mostly words)\n\nD.**Vocabulary** = collection of unique words of a *Corpus*.\n\nE.**D**ocument-**F**eature-**M**atrix or **D**ocument-**T**erm-**M**atrix =\n\nthis is a matrix where the rows indicate the documents, the columns indicate the expressions and the cells indicate the frequency of the expressions in each document, i.e.: Matrix with *n* = number of Document rows and *m* = size of Vocabulary columns, based on **bag-of-words** assumption, which ignores word order and syntax.\n\n##### 8.1. Pre-processing\n\nA. **Tokenisation**\n\nB. **Remove** Stopwords\n\nC. **String** operations (punctuation, normalise URL).\n\nThe order of the application steps is crucial and should be guided by the research question.\n\n```{r Pre-processing Punctuation}\n# Clean @-Symbol\nmental$post_text<- gsub(\"@\\\\w+\", \"\", mental$post_text)\n\n# Clean Interpunction\nmental$post_text<- gsub(\"[[:punct:]]\", \"\", mental$post_text)\n\n# Clean Numbers\nmental$post_text <- gsub(\"[[:digit:]]\", \"\", mental$post_text)\n\n# Clean pictures\nmental$post_text<- gsub(\"pictwitter\\\\w+ *\", \"\", mental$post_text)\n```\n\n```{r Pre-processing html}\n#Clean HTML-Notation\nremove_html <- \"&amp;|&lt;|&gt;\" #&lt und &gt stehen für < und > und &amp für &\n\nmental_en <- mental %>% \n  select(post_id, post_text, user_id) %>% \n  mutate(text = stringr::str_remove_all(post_text, remove_html))\n```\n\nWith [`textclean`](https://github.com/trinker/textclean) further harmonisation can be done, e.g. replacing emojis with word equivalents.\n\n```{r, warning=FALSE}\nmental_en$text <- replace_emoji(mental_en$text)\n```\n\nOr normalizing colloquial expressions such as word elongations (e.g., \"Whyyyy\")\n\n```{r, warning=FALSE}\nmental_en$text <- replace_word_elongation(mental_en$text)\n```\n\nFrom the data frame, you can create a *Corpus* object, i.e. a collection of *documents* (tweets) and their *metadata*.\n\n```{r}\n#select unique tweet_ids\nunique <- mental_en %>% group_by(post_id) %>% unique()\n```\n\n```{r Corpus Objekt, results='hide'}\ntweets_en_corpus <- corpus(unique,\n                           docid_field = \"post_id\",\n                           text_field = \"post_text\")\n\n```\n\nIn `quanteda` the smallest meaningful unit are **tokens** which can be generated by segmenting into words (see `tidytext` `unnest_tokens(word, text)`. Further, we can remove **stopwords** (hedges, grammatical signifiers which bear little semantic meaning). See (`stopwords()`)\n\n```{r Tokenisation, results=FALSE}\ntweets_en_tokens <- tokens(tweets_en_corpus,\n                           remove_punct = TRUE,\n                           remove_numbers = TRUE,\n                           remove_symbols = TRUE,\n                           remove_url = TRUE) %>% \n  tokens_tolower() %>% \n  tokens_remove(stopwords(\"english\"))\n```\n\n```{r KWIC, results='hide'}\n\nkw_depression <- kwic(tweets_en_tokens, pattern =  \"depress*\", window = 3)\nhead(kw_depression)\n```\n\n#### 8.2. Tidy Workflow--Exploration Tweets, Retweets and Hashtags\n\n```{r Tokenisation und Pre-processing}\n#Tokenising Tweets (1-gram) and Pre-processing with tidytext\n\ntweet_words <- mental %>%\n  select(user_id, \n         post_text, \n         retweets, \n         favourites,\n         formatted_date, \n         post_id) %>%\n  unnest_tokens(word, post_text) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  filter(!word %in% c(\"amp\", \"de\", \"twitter\", \"youre\", \"en\", \"die\", \"rt\", \"im\", \"dont\", \"ive\", \"yong\"),\n         stringr::str_detect(word, \"[a-z]\"))\n\ntweet_words %>%\n  count(word, sort = TRUE) %>%\n  head(16) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot2::ggplot(aes(word, n)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Most common words in Tweets on Depression\",\n       y = \"Frequency of words\")\n\n```\n\n```{r Favorites und Retweets, warning=FALSE, message=FALSE}\n#Distribution of Favorites\nmental %>%\n  ggplot2::ggplot(aes(favourites + 1)) +\n  geom_histogram() +\n  scale_x_log10()\n\n#Mean Retweets and Mean Favorites\nword_summary <- tweet_words  %>%\n  group_by(word) %>%\n  summarize(n = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1,\n            avg_favorites = exp(mean(log(favourites + 1))) - 1) %>%\n  filter(n >= 30) %>%\n  arrange(desc(avg_retweets))\n\n```\n\n```{r tf-idf}\n#Tf-idf: signals importance of a word in a corpus selection\n\ntop_word <- tweet_words %>%\n  count(word, formatted_date) %>%\n  bind_tf_idf(word, formatted_date, n) %>%\n  arrange(desc(tf_idf)) %>%\n  distinct(formatted_date, .keep_all = TRUE) %>%\n  arrange(formatted_date)\n\ntf_idf <- word_summary %>%\n  inner_join(top_word, by = c(\"word\")) %>%\n  arrange(desc(avg_retweets)) \n\n#Tf-idf of words per hour\ntf_idf %>% \n  ggplot2::ggplot(aes(tf_idf, forcats::fct_reorder(word, tf_idf), fill = word)) +\n  geom_col(show.legend = FALSE)\n```\n\n```{r hashtags, warning=FALSE}\n#Hashtag exploration\ntweet_words <- mental %>%\n  mutate(hashtags = stringr::str_count(post_text, \"#[a-zA-Z]\"), sort = TRUE) %>%\n  select(user_id, post_text, retweets, favourites, formatted_date , post_id,\n        hashtags) %>%\n  unnest_tokens(word, post_text) %>%\n  anti_join(stop_words, by = \"word\") %>%\n  filter(!word %in% c(\"de\", \"|\"),\n         stringr::str_detect(word, \"[a-z]\"))\n\n\nmental %>%\n  mutate(hashtags = stringr::str_count(post_text, \"#[a-zA-Z]\"), sort = TRUE) %>%\n  filter(hashtags < 6) %>%\n  group_by(user_id) %>%\n  summarize(tweets = n(),\n            avg_retweets = exp(mean(log(retweets + 1))) - 1) %>%\n  filter(tweets >= 30) %>%\n  arrange(desc(avg_retweets))\n\n\ntweet_word_summary <- tweet_words %>%\n  filter(hashtags < 6) %>%\n  group_by(word) %>%\n  summarize(n = n(),\n            avg_retweets = exp(mean(log(retweets+ 1))) - 1,\n            avg_favorites = exp(mean(log(favourites + 1))) - 1)\n\n\n\ntweet_word_summary <-tweet_word_summary %>%\n  filter(n >= 100,\n         !stringr::str_detect(word, \"https\")) %>%\n   filter(!word %in% c(\"amp\", \"de\", \"twitter\", \"youre\", \"en\", \"die\", \"rt\", \"im\", \"dont\", \"ive\", \"yong\"),\n         stringr::str_detect(word, \"[a-z]\")) %>% \n  arrange(desc(avg_retweets)) %>% \n  head()\n\ntweet_word_summary %>%\n  filter(n >= 100,\n         !stringr::str_detect(word, \"https\")) %>%\n  ggplot2::ggplot(aes(n, avg_retweets)) +\n  geom_point() +\n  geom_text(aes(label = word), check_overlap = TRUE) +\n  scale_x_log10() +\n  scale_y_log10()\n\ntweet_word_summary %>%\n  filter(n >= 100,\n         !stringr::str_detect(word, \"https\")) %>%\n  arrange(desc(avg_retweets)) %>%\n  head(20) %>%\n  mutate(word = reorder(word, avg_retweets)) %>%\n  ggplot2::ggplot(aes(word, avg_retweets)) +\n  geom_col() +\n  coord_flip() +\n  labs(title = \"Which words get the most retweets in depression?\",\n       subtitle = \"Only words appearing in at least 100 tweets\",\n       y = \"Geometric mean of the Number of retweets\")\n```\n\n\n\n# Predicting depression tweets based on followers and friends\n```{r warning=FALSE, message=FALSE}\n\n# Data preprocessing\n# Check for missing values\nmissing_values <- sum(is.na(mental))\nif (missing_values > 0) {\n  data <- na.omit(mental)  # Remove rows with missing values\n}\n\n# Convert label variable to factor\n#mental$label <- as.factor(mental$label)\n\n# Standardize the followers and friends variables\nmental$followers_standardized <- as.numeric(scale(mental$followers))\nmental$friends_standardized <- as.numeric(scale(mental$friends))\n\n# Check the standardized variables\nhead(mental[, c(\"followers\", \"followers_standardized\", \"friends\", \"friends_standardized\")])\n\n\n# Select variables for analysis\nselected_vars <- c(\"label\", \"followers_standardized\", \"friends_standardized\")\nmental_selected <- mental[selected_vars]\n\n\n# Split the dataset into training and testing sets\nset.seed(123)  # Set a seed for reproducibility\ntrain_indices <- createDataPartition(mental_selected$label, p = 0.8, list = FALSE)\ntrain_data <- mental_selected[train_indices, ]\ntest_data <- mental_selected[-train_indices, ]\n\n\ntrain_data$label <- as.numeric(train_data$label)\ntest_data$label <- as.numeric(test_data$label)\n\n# Linear Regression model\nmodel <- lm(label ~ followers_standardized + friends_standardized, data = train_data)\n\n# Model evaluation\npredictions <- predict(model, newdata = test_data)\nevaluation <- data.frame(actual = test_data$label, predicted = predictions)\n\n# Calculate evaluation metrics\nrmse <- mean((evaluation$actual - evaluation$predicted)^2)\nrmse <- sqrt(rmse)\nr_squared <- cor(evaluation$actual, evaluation$predicted)^2\n\n# Print evaluation metrics\ncat(\"Mean Squared Error (MSE):\", rmse, \"\\n\")\ncat(\"Root Mean Squared Error (RMSE):\", rmse, \"\\n\")\ncat(\"R-squared value:\", r_squared, \"\\n\")\n\n# Regression coefficients and p-values\nsummary(model)\n\nlibrary(dotwhisker)\ndwplot(model)+  \n   xlab(\"Coefficient Estimate\") + ylab(\"\") +\n    geom_vline(xintercept = 0,\n               colour = \"grey60\",\n               linetype = 2) +\n  ggtitle(\"Predicting Depression Tweets\") \n\n```\n\nThe followers_standardized coefficient (-0.286159) indicates that, on average, a one-standard-deviation increase in the standardized followers variable is associated with a decrease of 0.286159 units in the predicted depression label.\nThe friends_standardized coefficient (0.314163) indicates that, on average, a one-standard-deviation increase in the standardized friends variable is associated with an increase of 0.314163 units in the predicted depression label. All t-values are large (absolute values > 2), indicating that the coefficients are significantly different from zero.\n\n\nOverall, the regression analysis suggests that both the standardized followers and standardized friends variables have a statistically significant association with the predicted depression label. An increase in the standardized followers is associated with a decrease in the predicted depression label, while an increase in the standardized friends is associated with an increase in the predicted depression label.\n\n\n```{r}\n# Fit a non-linear regression model\nmodel_nonlinear <- nls(label ~ a * exp(b * followers_standardized) + c * friends_standardized,\n                       data = train_data,\n                       start = list(a = 1, b = 1, c = 1))\n\n# Print the model summary\nsummary(model_nonlinear)\n\n# Predict the label using the non-linear regression model\ntest_data$predicted_nonlinear <- predict(model_nonlinear, newdata = test_data)\n\n# Evaluate the non-linear regression model\nmse_nonlinear <- mean((test_data$label - test_data$predicted_nonlinear)^2)\nrmse_nonlinear <- sqrt(mse_nonlinear)\nr_squared_nonlinear <- 1 - sum((test_data$label - test_data$predicted_nonlinear)^2) / sum((test_data$label - mean(test_data$label))^2)\n\n# Print evaluation metrics\ncat(\"Mean Squared Error (MSE) - Nonlinear Regression:\", mse_nonlinear, \"\\n\")\ncat(\"Root Mean Squared Error (RMSE) - Nonlinear Regression:\", rmse_nonlinear, \"\\n\")\ncat(\"R-squared value - Nonlinear Regression:\", r_squared_nonlinear, \"\\n\")\n\n```\n\n```{r}\ndwplot(model_nonlinear)+  \n   xlab(\"Coefficient Estimate\") + ylab(\"\") +\n    geom_vline(xintercept = 0,\n               colour = \"grey60\",\n               linetype = 2) +\n  ggtitle(\"Predicting Depression Tweets with a non-linear model\") \n\n```\n\n\n```{r}\n\nlibrary(gam)\nlibrary(mgcv)\n\n# Fit a non-linear regression model using GAM\nmodel_gam <- gam(label ~ s(followers_standardized) + s(friends_standardized), data = train_data)\n\n# Preprocess data by removing missing values\ncomplete_data <- train_data[complete.cases(train_data), ]\n\n\n# Visualize the estimated effects of the variables\npar(mfrow = c(1, 2))  # Adjust the layout of the plots if desired\n\n# Plot the estimated effect of 'followers'\nplot(model_gam, select = 1, se=TRUE, residuals = TRUE,\n     xlab = \"Followers\", ylab = \"Effect of Followers on Label\")\n\n# Plot the estimated effect of 'friends'\nplot(model_gam, select = 2, se=TRUE, residuals = TRUE,\n     xlab = \"Friends\", ylab = \"Effect of Friends on Label\")\n\n\n\n```\n```{r}\nlibrary('gratia')\n# plot all smooths\ndraw(model_gam)\n```\n\nAdding partial residuals to the plot of the estimated effects in a GAM model provides a way to assess the goodness of fit and the presence of systematic patterns in the model.\n\nPartial residuals are calculated by subtracting the estimated component of the model (excluding the effect of a specific variable of interest) from the observed response variable. These residuals represent the part of the response variable that is not accounted for by the model for a specific variable.\n\nWhen partial residuals are added to the plot of the estimated effects, they are typically shown as individual points or a smooth line against the corresponding variable. This allows you to visually inspect the relationship between the variable and the unexplained variation in the response variable, which can help identify any remaining patterns or trends not captured by the model.\n\nBy examining the distribution and pattern of the partial residuals, you can assess whether the model captures the underlying structure and if there are any systematic deviations from the model's predictions. Systematic patterns in the partial residuals may indicate that the model is misspecified or that there are additional variables or nonlinear relationships that should be considered.\n\n\n```{r}\ndraw(model_gam, residuals = TRUE)\n```\n\n\n\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"atom-one","output-file":"summary.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","bibliography":["references.bib"],"number-depth":3,"theme":"cosmo","code-block-bg":"#f0f0f0","code-block-border-left":"#045a8d","code-summary":"Code","code-copy":true,"knitr":{"opts_chunk":{"out.width":"100%","R.options":{"width":68}}}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}